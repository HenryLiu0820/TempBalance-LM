{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "file_path = '/scratch/tpang/zhliu/checkpoints/nlp/mt/iwslt14_de_en/baseline/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/train_log.txt'\n",
    "\n",
    "def parse_line_updates(line):\n",
    "    # Updated regular expression to match exponential format for 'lr'\n",
    "    pattern = r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2} \\|\\s*INFO\\s*\\|\\s*train_inner\\s*\\|)?\\s*epoch\\s*(\\d+):\\s*(\\d+) \\/ (\\d+)\\s*loss=(\\d+\\.\\d+), nll_loss=(\\d+\\.\\d+), ppl=(\\d+\\.\\d+), wps=(\\d+\\.\\d+), ups=(\\d+\\.\\d+), wpb=(\\d+\\.\\d+), bsz=(\\d+\\.\\d+), num_updates=(\\d+), lr=([\\d\\.]+e?[-\\+]?\\d*), gnorm=(\\d+\\.\\d+), train_wall=(\\d+), (gb_free=(\\d+\\.\\d+),)?\\s*wall=(\\d+)'\n",
    "    match = re.search(pattern, line)\n",
    "    if match:\n",
    "        return {\n",
    "            'epoch': int(match.group(1)),\n",
    "            'loss': float(match.group(2)),\n",
    "            'nll_loss': float(match.group(3)),\n",
    "            'ppl': float(match.group(4)),\n",
    "            'wps': int(match.group(5)),\n",
    "            'ups': int(match.group(6)),\n",
    "            'wpb': float(match.group(7)),\n",
    "            'bsz': float(match.group(8)),\n",
    "            'num_updates': int(match.group(9)),\n",
    "            'lr': float(match.group(10)),  # Converts exponential format to float\n",
    "            'gnorm': float(match.group(11)),\n",
    "            'clip': float(match.group(12)),\n",
    "            'oom': float(match.group(13)),\n",
    "            'wall': int(match.group(14)),\n",
    "            'train_wall': int(match.group(15))\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def parse_line_epochs(line):\n",
    "    # Regular expression for the new format, supporting 'lr' in exponential format\n",
    "    pattern = r'\\|\\s*epoch\\s*(\\d+)\\s*\\|\\s*loss\\s*(\\d+\\.\\d+)\\s*\\|\\s*nll_loss\\s*(\\d+\\.\\d+)\\s*\\|\\s*ppl\\s*(\\d+\\.\\d+)\\s*\\|\\s*wps\\s*(\\d+)\\s*\\|\\s*ups\\s*(\\d+)\\s*\\|\\s*wpb\\s*(\\d+\\.\\d+)\\s*\\|\\s*bsz\\s*(\\d+\\.\\d+)\\s*\\|\\s*num_updates\\s*(\\d+)\\s*\\|\\s*lr\\s*([\\d\\.]+e?[-\\+]?\\d*)\\s*\\|\\s*gnorm\\s*(\\d+\\.\\d+)\\s*\\|\\s*clip\\s*(\\d+\\.\\d+)\\s*\\|\\s*oom\\s*(\\d+\\.\\d+)\\s*\\|\\s*wall\\s*(\\d+)\\s*\\|\\s*train_wall\\s*(\\d+)'\n",
    "    match = re.search(pattern, line)\n",
    "    if match:\n",
    "        return {\n",
    "            'epoch': int(match.group(1)),\n",
    "            'loss': float(match.group(2)),\n",
    "            'nll_loss': float(match.group(3)),\n",
    "            'ppl': float(match.group(4)),\n",
    "            'wps': int(match.group(5)),\n",
    "            'ups': int(match.group(6)),\n",
    "            'wpb': float(match.group(7)),\n",
    "            'bsz': float(match.group(8)),\n",
    "            'num_updates': int(match.group(9)),\n",
    "            'lr': float(match.group(10)),\n",
    "            'gnorm': float(match.group(11)),\n",
    "            'clip': float(match.group(12)),\n",
    "            'oom': float(match.group(13)),\n",
    "            'wall': int(match.group(14)),\n",
    "            'train_wall': int(match.group(15))\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def parse_line_validation_stats(line):\n",
    "    # Regular expression for parsing validation stats\n",
    "    pattern = r'\\|\\s*epoch\\s*(\\d+)\\s*\\|\\s*valid on \\'valid\\' subset\\s*\\|\\s*loss\\s*(\\d+\\.\\d+)\\s*\\|\\s*nll_loss\\s*(\\d+\\.\\d+)\\s*\\|\\s*ppl\\s*(\\d+\\.\\d+)\\s*\\|\\s*num_updates\\s*(\\d+)'\n",
    "    match = re.search(pattern, line)\n",
    "    if match:\n",
    "        return {\n",
    "            'epoch': int(match.group(1)),\n",
    "            'loss': float(match.group(2)),\n",
    "            'nll_loss': float(match.group(3)),\n",
    "            'ppl': float(match.group(4)),\n",
    "            'num_updates': int(match.group(5))\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def parse_file(file_path):\n",
    "    updates_data = []\n",
    "    epoch_data = []\n",
    "    val_data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parsed_line_updates = parse_line_updates(line)\n",
    "            if parsed_line_updates:\n",
    "                updates_data.append(parsed_line_updates)\n",
    "            parsed_line_epochs = parse_line_epochs(line)\n",
    "            if parsed_line_epochs:\n",
    "                epoch_data.append(parsed_line_epochs)\n",
    "            parsed_line_val = parse_line_validation_stats(line)\n",
    "            if parsed_line_val:\n",
    "                val_data.append(parsed_line_val)\n",
    "\n",
    "    return pd.DataFrame(updates_data), pd.DataFrame(epoch_data), pd.DataFrame(val_data)\n",
    "\n",
    "# Example usage\n",
    "df_updates, df_epochs, df_val = parse_file(file_path)\n",
    "# df.to_csv('output.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(      epoch    loss  nll_loss       ppl    wps  ups       wpb      bsz  \\\n",
       " 0         1  13.759    13.710  13397.50  23451    6  3630.902  161.412   \n",
       " 1         1  12.868    12.720   6748.83  23645    6  3634.762  166.168   \n",
       " 2         1  12.273    12.059   4266.72  23467    7  3604.808  168.894   \n",
       " 3         1  11.837    11.571   3042.18  23540    7  3608.935  160.990   \n",
       " 4         1  11.502    11.190   2336.03  23470    7  3595.649  152.729   \n",
       " ...     ...     ...       ...       ...    ...  ...       ...      ...   \n",
       " 1205     55   3.360     1.774      3.42  22475    6  3580.582  143.706   \n",
       " 1206     55   3.359     1.773      3.42  22518    6  3581.961  144.655   \n",
       " 1207     55   3.358     1.772      3.42  22585    6  3586.192  145.957   \n",
       " 1208     55   3.361     1.776      3.42  22264    6  3584.260  145.301   \n",
       " 1209     55   3.362     1.777      3.43  22332    6  3586.843  145.540   \n",
       " \n",
       "       num_updates        lr  gnorm  clip  oom  wall  train_wall  \n",
       " 0              51  0.000010  2.730   0.0  0.0    18          11  \n",
       " 1             101  0.000019  2.239   0.0  0.0    26          18  \n",
       " 2             151  0.000028  1.922   0.0  0.0    34          25  \n",
       " 3             201  0.000038  1.666   0.0  0.0    41          32  \n",
       " 4             251  0.000047  1.467   0.0  0.0    49          40  \n",
       " ...           ...       ...    ...   ...  ...   ...         ...  \n",
       " 1205        60355  0.000546  0.415   0.0  0.0  9870        9122  \n",
       " 1206        60405  0.000546  0.415   0.0  0.0  9878        9129  \n",
       " 1207        60455  0.000546  0.414   0.0  0.0  9885        9136  \n",
       " 1208        60505  0.000545  0.416   0.0  0.0  9896        9146  \n",
       " 1209        60555  0.000545  0.415   0.0  0.0  9903        9154  \n",
       " \n",
       " [1210 rows x 15 columns],\n",
       "     epoch   loss  nll_loss     ppl    wps  ups       wpb     bsz  num_updates  \\\n",
       " 0       1  9.428     8.782  440.11  22736    6  3586.843  145.54         1101   \n",
       " 1       2  7.295     6.316   79.68  22572    6  3586.843  145.54         2202   \n",
       " 2       3  6.119     4.944   30.79  22255    6  3586.843  145.54         3303   \n",
       " 3       4  5.386     4.088   17.01  22438    6  3586.843  145.54         4404   \n",
       " 4       5  5.052     3.705   13.04  22509    6  3586.843  145.54         5505   \n",
       " 5       6  4.865     3.493   11.26  22697    6  3586.843  145.54         6606   \n",
       " 6       7  4.739     3.351   10.20  22526    6  3586.843  145.54         7707   \n",
       " 7       8  4.618     3.215    9.29  22678    6  3586.843  145.54         8808   \n",
       " 8       9  4.457     3.034    8.19  22556    6  3586.843  145.54         9909   \n",
       " 9      10  4.330     2.890    7.41  22195    6  3586.843  145.54        11010   \n",
       " 10     11  4.233     2.781    6.87  22465    6  3586.843  145.54        12111   \n",
       " 11     12  4.154     2.691    6.46  22656    6  3586.843  145.54        13212   \n",
       " 12     13  4.086     2.614    6.12  22613    6  3586.843  145.54        14313   \n",
       " 13     14  4.030     2.550    5.86  22835    6  3586.843  145.54        15414   \n",
       " 14     15  3.980     2.493    5.63  22188    6  3586.843  145.54        16515   \n",
       " 15     16  3.936     2.443    5.44  22449    6  3586.843  145.54        17616   \n",
       " 16     17  3.899     2.401    5.28  22519    6  3586.843  145.54        18717   \n",
       " 17     18  3.863     2.359    5.13  22673    6  3586.843  145.54        19818   \n",
       " 18     19  3.831     2.322    5.00  22638    6  3586.843  145.54        20919   \n",
       " 19     20  3.804     2.291    4.89  22182    6  3586.843  145.54        22020   \n",
       " 20     21  3.776     2.259    4.79  22704    6  3586.843  145.54        23121   \n",
       " 21     22  3.751     2.230    4.69  22706    6  3586.843  145.54        24222   \n",
       " 22     23  3.728     2.203    4.61  22643    6  3586.843  145.54        25323   \n",
       " 23     24  3.707     2.180    4.53  22690    6  3586.843  145.54        26424   \n",
       " 24     25  3.686     2.155    4.45  22276    6  3586.843  145.54        27525   \n",
       " 25     26  3.669     2.134    4.39  22789    6  3586.843  145.54        28626   \n",
       " 26     27  3.649     2.111    4.32  22750    6  3586.843  145.54        29727   \n",
       " 27     28  3.632     2.092    4.26  22730    6  3586.843  145.54        30828   \n",
       " 28     29  3.615     2.073    4.21  22771    6  3586.843  145.54        31929   \n",
       " 29     30  3.600     2.055    4.16  22441    6  3586.843  145.54        33030   \n",
       " 30     31  3.587     2.039    4.11  22671    6  3586.843  145.54        34131   \n",
       " 31     32  3.573     2.024    4.07  22545    6  3586.843  145.54        35232   \n",
       " 32     33  3.560     2.008    4.02  22653    6  3586.843  145.54        36333   \n",
       " 33     34  3.546     1.993    3.98  22610    6  3586.843  145.54        37434   \n",
       " 34     35  3.534     1.979    3.94  22421    6  3586.843  145.54        38535   \n",
       " 35     36  3.524     1.967    3.91  22684    6  3586.843  145.54        39636   \n",
       " 36     37  3.510     1.951    3.87  22645    6  3586.843  145.54        40737   \n",
       " 37     38  3.500     1.939    3.83  22619    6  3586.843  145.54        41838   \n",
       " 38     39  3.491     1.928    3.81  22648    6  3586.843  145.54        42939   \n",
       " 39     40  3.480     1.915    3.77  22273    6  3586.843  145.54        44040   \n",
       " 40     41  3.471     1.904    3.74  22673    6  3586.843  145.54        45141   \n",
       " 41     42  3.461     1.893    3.71  22859    6  3586.843  145.54        46242   \n",
       " 42     43  3.452     1.882    3.69  22804    6  3586.843  145.54        47343   \n",
       " 43     44  3.444     1.873    3.66  22845    6  3586.843  145.54        48444   \n",
       " 44     45  3.434     1.861    3.63  22371    6  3586.843  145.54        49545   \n",
       " 45     46  3.428     1.854    3.62  22756    6  3586.843  145.54        50646   \n",
       " 46     47  3.420     1.845    3.59  22655    6  3586.843  145.54        51747   \n",
       " 47     48  3.410     1.833    3.56  22709    6  3586.843  145.54        52848   \n",
       " 48     49  3.405     1.827    3.55  22681    6  3586.843  145.54        53949   \n",
       " 49     50  3.397     1.817    3.52  22377    6  3586.843  145.54        55050   \n",
       " 50     51  3.390     1.809    3.50  22727    6  3586.843  145.54        56151   \n",
       " 51     52  3.382     1.800    3.48  22797    6  3586.843  145.54        57252   \n",
       " 52     53  3.374     1.791    3.46  22753    6  3586.843  145.54        58353   \n",
       " 53     54  3.370     1.786    3.45  22692    6  3586.843  145.54        59454   \n",
       " 54     55  3.362     1.777    3.43  22325    6  3586.843  145.54        60555   \n",
       " \n",
       "           lr  gnorm  clip  oom  wall  train_wall  \n",
       " 0   0.000207  1.015   0.0  0.0   184         169  \n",
       " 1   0.000413  0.898   0.0  0.0   363         335  \n",
       " 2   0.000619  0.858   0.0  0.0   546         503  \n",
       " 3   0.000826  0.798   0.0  0.0   728         670  \n",
       " 4   0.001032  0.714   0.0  0.0   909         837  \n",
       " 5   0.001239  0.622   0.0  0.0  1088        1003  \n",
       " 6   0.001445  0.554   0.0  0.0  1269        1170  \n",
       " 7   0.001430  0.484   0.0  0.0  1449        1336  \n",
       " 8   0.001348  0.438   0.0  0.0  1629        1502  \n",
       " 9   0.001279  0.411   0.0  0.0  1813        1671  \n",
       " 10  0.001219  0.397   0.0  0.0  1994        1838  \n",
       " 11  0.001167  0.390   0.0  0.0  2174        2004  \n",
       " 12  0.001121  0.385   0.0  0.0  2354        2171  \n",
       " 13  0.001081  0.380   0.0  0.0  2532        2335  \n",
       " 14  0.001044  0.380   0.0  0.0  2716        2505  \n",
       " 15  0.001011  0.376   0.0  0.0  2897        2672  \n",
       " 16  0.000981  0.377   0.0  0.0  3078        2839  \n",
       " 17  0.000953  0.378   0.0  0.0  3258        3005  \n",
       " 18  0.000928  0.377   0.0  0.0  3438        3171  \n",
       " 19  0.000904  0.377   0.0  0.0  3622        3340  \n",
       " 20  0.000882  0.379   0.0  0.0  3802        3506  \n",
       " 21  0.000862  0.378   0.0  0.0  3981        3671  \n",
       " 22  0.000843  0.380   0.0  0.0  4161        3837  \n",
       " 23  0.000825  0.381   0.0  0.0  4340        4003  \n",
       " 24  0.000809  0.382   0.0  0.0  4522        4172  \n",
       " 25  0.000793  0.384   0.0  0.0  4700        4337  \n",
       " 26  0.000778  0.383   0.0  0.0  4880        4502  \n",
       " 27  0.000764  0.384   0.0  0.0  5059        4668  \n",
       " 28  0.000751  0.382   0.0  0.0  5238        4833  \n",
       " 29  0.000738  0.386   0.0  0.0  5419        5001  \n",
       " 30  0.000726  0.389   0.0  0.0  5598        5167  \n",
       " 31  0.000715  0.388   0.0  0.0  5779        5333  \n",
       " 32  0.000704  0.390   0.0  0.0  5958        5499  \n",
       " 33  0.000693  0.391   0.0  0.0  6138        5665  \n",
       " 34  0.000683  0.393   0.0  0.0  6319        5833  \n",
       " 35  0.000674  0.392   0.0  0.0  6497        5999  \n",
       " 36  0.000665  0.394   0.0  0.0  6678        6165  \n",
       " 37  0.000656  0.393   0.0  0.0  6858        6331  \n",
       " 38  0.000647  0.399   0.0  0.0  7037        6497  \n",
       " 39  0.000639  0.397   0.0  0.0  7219        6666  \n",
       " 40  0.000631  0.399   0.0  0.0  7398        6831  \n",
       " 41  0.000624  0.400   0.0  0.0  7575        6996  \n",
       " 42  0.000617  0.402   0.0  0.0  7753        7161  \n",
       " 43  0.000610  0.403   0.0  0.0  7930        7325  \n",
       " 44  0.000603  0.404   0.0  0.0  8111        7494  \n",
       " 45  0.000596  0.406   0.0  0.0  8289        7659  \n",
       " 46  0.000590  0.406   0.0  0.0  8469        7825  \n",
       " 47  0.000584  0.408   0.0  0.0  8648        7990  \n",
       " 48  0.000578  0.414   0.0  0.0  8826        8156  \n",
       " 49  0.000572  0.410   0.0  0.0  9007        8324  \n",
       " 50  0.000566  0.411   0.0  0.0  9185        8489  \n",
       " 51  0.000561  0.410   0.0  0.0  9363        8654  \n",
       " 52  0.000555  0.412   0.0  0.0  9542        8819  \n",
       " 53  0.000550  0.414   0.0  0.0  9721        8985  \n",
       " 54  0.000545  0.415   0.0  0.0  9903        9154  ,\n",
       "     epoch   loss  nll_loss     ppl  num_updates\n",
       " 0       1  7.788     6.847  115.14         1101\n",
       " 1       2  6.445     5.270   38.57         2202\n",
       " 2       3  5.368     3.985   15.83         3303\n",
       " 3       4  4.918     3.451   10.94         4404\n",
       " 4       5  4.755     3.257    9.56         5505\n",
       " 5       6  4.582     3.056    8.32         6606\n",
       " 6       7  4.508     3.006    8.03         7707\n",
       " 7       8  4.398     2.847    7.19         8808\n",
       " 8       9  4.290     2.742    6.69         9909\n",
       " 9      10  4.192     2.644    6.25        11010\n",
       " 10     11  4.172     2.588    6.01        12111\n",
       " 11     12  4.117     2.551    5.86        13212\n",
       " 12     13  4.061     2.487    5.61        14313\n",
       " 13     14  4.041     2.462    5.51        15414\n",
       " 14     15  4.026     2.441    5.43        16515\n",
       " 15     16  4.016     2.420    5.35        17616\n",
       " 16     17  3.992     2.400    5.28        18717\n",
       " 17     18  3.982     2.388    5.24        19818\n",
       " 18     19  3.959     2.360    5.13        20919\n",
       " 19     20  3.955     2.357    5.12        22020\n",
       " 20     21  3.944     2.344    5.08        23121\n",
       " 21     22  3.920     2.326    5.01        24222\n",
       " 22     23  3.921     2.319    4.99        25323\n",
       " 23     24  3.910     2.305    4.94        26424\n",
       " 24     25  3.924     2.318    4.99        27525\n",
       " 25     26  3.902     2.296    4.91        28626\n",
       " 26     27  3.900     2.294    4.90        29727\n",
       " 27     28  3.897     2.292    4.90        30828\n",
       " 28     29  3.890     2.282    4.86        31929\n",
       " 29     30  3.901     2.285    4.87        33030\n",
       " 30     31  3.886     2.276    4.84        34131\n",
       " 31     32  3.895     2.282    4.86        35232\n",
       " 32     33  3.878     2.263    4.80        36333\n",
       " 33     34  3.882     2.266    4.81        37434\n",
       " 34     35  3.884     2.267    4.81        38535\n",
       " 35     36  3.873     2.261    4.79        39636\n",
       " 36     37  3.862     2.249    4.75        40737\n",
       " 37     38  3.868     2.253    4.77        41838\n",
       " 38     39  3.868     2.256    4.78        42939\n",
       " 39     40  3.880     2.259    4.79        44040\n",
       " 40     41  3.874     2.260    4.79        45141\n",
       " 41     42  3.874     2.254    4.77        46242\n",
       " 42     43  3.868     2.250    4.76        47343\n",
       " 43     44  3.876     2.254    4.77        48444\n",
       " 44     45  3.887     2.266    4.81        49545\n",
       " 45     46  3.862     2.246    4.74        50646\n",
       " 46     47  3.865     2.246    4.74        51747\n",
       " 47     48  3.864     2.249    4.75        52848\n",
       " 48     49  3.871     2.250    4.76        53949\n",
       " 49     50  3.875     2.252    4.76        55050\n",
       " 50     51  3.863     2.247    4.75        56151\n",
       " 51     52  3.861     2.245    4.74        57252\n",
       " 52     53  3.894     2.270    4.82        58353\n",
       " 53     54  3.847     2.237    4.72        59454\n",
       " 54     55  3.863     2.243    4.74        60555)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_updates, df_epochs, df_val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ww_train_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
