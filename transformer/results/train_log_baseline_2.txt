Namespace(acc_block='True', acc_mid='False', acc_v='False', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer_iwslt_de_en_v2', attention_dropout=0.1, batchnorm='True', best_checkpoint_metric='loss', bpe=None, bucket_cap_mb=25, clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/yefan0726/data/nlp/mt/data-bin/iwslt14.tokenized.de-en.joined', dataset_impl=None, ddp_backend='no_c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_norm_ff='layer', decoder_norm_self='layer', decoder_normalize_before=True, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, dropout_gama=0.5, dropout_type='none', early_stop=10000, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_norm_ff='layer', encoder_norm_self='layer', encoder_normalize_before=True, encoder_spec_norm=False, esd_interval=500, fast_stat_sync=False, filter_zeros='False', find_unused_parameters=False, fix_batches_to_gpus=False, fix_fingers='xmin_mid', fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=5, label_smoothing=0.1, layer_wise_attention=False, lazy_load=False, left_pad_source='True', left_pad_target='False', load_alignments=False, log_format=None, log_interval=50, lr=[0.0015], lr_min_ratio=0.6, lr_scale=2.0, lr_scheduler='inverse_sqrt', lr_slope=0.8, max_epoch=55, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, metric='alpha', min_loss_scale=0.0001, min_lr=1e-09, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_token_positional_embeddings=False, num_workers=1, optimizer='adam', optimizer_overrides='{}', raw_text=False, relu_dropout=0.1, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='/data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0/checkpoint_best.pt', save_dir='/data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0', save_interval=1, save_interval_updates=0, seed=43, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, single_gpu=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tb='False', tbmf_wrapper=False, tbr_after_warm='True', temp_balance_lr='tb_linear_map', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', update_freq=[1], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_init_lr=1e-07, warmup_updates=8000, weight_decay=0.0001, xmin_pos=2.0)
| [de] dictionary: 10152 types
| [en] dictionary: 10152 types
| loaded 7283 examples from: /data/yefan0726/data/nlp/mt/data-bin/iwslt14.tokenized.de-en.joined/valid.de-en.de
| loaded 7283 examples from: /data/yefan0726/data/nlp/mt/data-bin/iwslt14.tokenized.de-en.joined/valid.de-en.en
| /data/yefan0726/data/nlp/mt/data-bin/iwslt14.tokenized.de-en.joined valid de-en 7283 examples
| model transformer_iwslt_de_en_v2, criterion LabelSmoothedCrossEntropyCriterion
| num. model params: 36743168 (num. trained: 36743168)
---------------------> Trainer: save_dir /data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0
| training on 1 GPUs
| max tokens per GPU = 4096 and max sentences per GPU = None
-------------------->file checkpoint utils load checkpoint
--------------------> Trainer.py: load_checkpoint
--------------------> bexists False
| no existing checkpoint found /data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0/checkpoint_best.pt
| loading train data for epoch 0
| loaded 160239 examples from: /data/yefan0726/data/nlp/mt/data-bin/iwslt14.tokenized.de-en.joined/train.de-en.de
| loaded 160239 examples from: /data/yefan0726/data/nlp/mt/data-bin/iwslt14.tokenized.de-en.joined/train.de-en.en
| /data/yefan0726/data/nlp/mt/data-bin/iwslt14.tokenized.de-en.joined train de-en 160239 examples
--------------------> property _lr_scheduler: self._build_optimizer()
--------------------> _build_optimizer
Baseline INIT
| NOTICE: your device may support faster training with --fp16
#############Initialize this optimizer!!!!!
--------------------> Initialize Adam Optimizer
We should initialize the learning rate scheduler immediately
--------------------> Initialize the InverseSquareRootSchedule
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
| epoch 001:     50 / 1101 loss=13.757, nll_loss=13.708, ppl=13377.40, wps=18547, ups=5, wpb=3630.902, bsz=161.412, num_updates=51, lr=9.66186e-06, gnorm=2.738, clip=0.000, oom=0.000, wall=21, train_wall=11
| epoch 001:    100 / 1101 loss=12.863, nll_loss=12.715, ppl=6723.15, wps=18723, ups=5, wpb=3634.762, bsz=166.168, num_updates=101, lr=1.90362e-05, gnorm=2.241, clip=0.000, oom=0.000, wall=31, train_wall=20
| epoch 001:    150 / 1101 loss=12.269, nll_loss=12.053, ppl=4250.12, wps=18789, ups=5, wpb=3604.808, bsz=168.894, num_updates=151, lr=2.84106e-05, gnorm=1.923, clip=0.000, oom=0.000, wall=40, train_wall=29
| epoch 001:    200 / 1101 loss=11.833, nll_loss=11.566, ppl=3032.00, wps=18859, ups=5, wpb=3608.935, bsz=160.990, num_updates=201, lr=3.7785e-05, gnorm=1.666, clip=0.000, oom=0.000, wall=49, train_wall=38
| epoch 001:    250 / 1101 loss=11.498, nll_loss=11.185, ppl=2328.97, wps=18882, ups=5, wpb=3595.649, bsz=152.729, num_updates=251, lr=4.71594e-05, gnorm=1.467, clip=0.000, oom=0.000, wall=59, train_wall=47
| epoch 001:    300 / 1101 loss=11.223, nll_loss=10.868, ppl=1869.02, wps=18919, ups=5, wpb=3589.761, bsz=149.073, num_updates=301, lr=5.65337e-05, gnorm=1.339, clip=0.000, oom=0.000, wall=68, train_wall=56
| epoch 001:    350 / 1101 loss=11.000, nll_loss=10.609, ppl=1561.42, wps=18974, ups=5, wpb=3596.761, bsz=145.137, num_updates=351, lr=6.59081e-05, gnorm=1.240, clip=0.000, oom=0.000, wall=77, train_wall=66
| epoch 001:    400 / 1101 loss=10.820, nll_loss=10.398, ppl=1348.85, wps=18976, ups=5, wpb=3598.117, bsz=144.536, num_updates=401, lr=7.52825e-05, gnorm=1.202, clip=0.000, oom=0.000, wall=87, train_wall=75
| epoch 001:    450 / 1101 loss=10.673, nll_loss=10.227, ppl=1198.20, wps=18714, ups=5, wpb=3581.792, bsz=142.898, num_updates=451, lr=8.46569e-05, gnorm=1.157, clip=0.000, oom=0.000, wall=97, train_wall=85
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
| epoch 001:    500 / 1101 loss=10.527, nll_loss=10.056, ppl=1064.48, wps=17414, ups=5, wpb=3585.122, bsz=143.806, num_updates=501, lr=9.40312e-05, gnorm=1.130, clip=0.000, oom=0.000, wall=114, train_wall=101
| epoch 001:    550 / 1101 loss=10.388, nll_loss=9.895, ppl=951.83, wps=17371, ups=5, wpb=3595.494, bsz=144.768, num_updates=551, lr=0.000103406, gnorm=1.104, clip=0.000, oom=0.000, wall=125, train_wall=112
| epoch 001:    600 / 1101 loss=10.260, nll_loss=9.747, ppl=859.16, wps=17292, ups=5, wpb=3597.203, bsz=146.221, num_updates=601, lr=0.00011278, gnorm=1.088, clip=0.000, oom=0.000, wall=136, train_wall=123
| epoch 001:    650 / 1101 loss=10.162, nll_loss=9.632, ppl=793.72, wps=17253, ups=5, wpb=3588.195, bsz=145.621, num_updates=651, lr=0.000122154, gnorm=1.091, clip=0.000, oom=0.000, wall=146, train_wall=133
| epoch 001:    700 / 1101 loss=10.058, nll_loss=9.512, ppl=729.91, wps=17288, ups=5, wpb=3586.425, bsz=146.258, num_updates=701, lr=0.000131529, gnorm=1.076, clip=0.000, oom=0.000, wall=156, train_wall=142
| epoch 001:    750 / 1101 loss=9.960, nll_loss=9.398, ppl=674.56, wps=17367, ups=5, wpb=3586.947, bsz=147.130, num_updates=751, lr=0.000140903, gnorm=1.072, clip=0.000, oom=0.000, wall=166, train_wall=152
| epoch 001:    800 / 1101 loss=9.876, nll_loss=9.300, ppl=630.25, wps=17496, ups=5, wpb=3593.109, bsz=145.277, num_updates=801, lr=0.000150277, gnorm=1.053, clip=0.000, oom=0.000, wall=175, train_wall=161
| epoch 001:    850 / 1101 loss=9.783, nll_loss=9.193, ppl=585.19, wps=17606, ups=5, wpb=3597.162, bsz=146.744, num_updates=851, lr=0.000159652, gnorm=1.054, clip=0.000, oom=0.000, wall=185, train_wall=170
| epoch 001:    900 / 1101 loss=9.706, nll_loss=9.103, ppl=549.90, wps=17678, ups=5, wpb=3593.454, bsz=146.139, num_updates=901, lr=0.000169026, gnorm=1.041, clip=0.000, oom=0.000, wall=194, train_wall=179
| epoch 001:    950 / 1101 loss=9.632, nll_loss=9.018, ppl=518.38, wps=17755, ups=5, wpb=3591.905, bsz=145.850, num_updates=951, lr=0.000178401, gnorm=1.040, clip=0.000, oom=0.000, wall=203, train_wall=188
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
| epoch 001:   1000 / 1101 loss=9.564, nll_loss=8.938, ppl=490.58, wps=17314, ups=5, wpb=3588.978, bsz=144.958, num_updates=1001, lr=0.000187775, gnorm=1.033, clip=0.000, oom=0.000, wall=218, train_wall=203
| epoch 001:   1050 / 1101 loss=9.490, nll_loss=8.853, ppl=462.39, wps=17415, ups=5, wpb=3593.131, bsz=145.369, num_updates=1051, lr=0.000197149, gnorm=1.025, clip=0.000, oom=0.000, wall=228, train_wall=212
| epoch 001:   1100 / 1101 loss=9.426, nll_loss=8.779, ppl=439.35, wps=17465, ups=5, wpb=3586.843, bsz=145.540, num_updates=1101, lr=0.000206524, gnorm=1.020, clip=0.000, oom=0.000, wall=237, train_wall=221
| epoch 001 | loss 9.426 | nll_loss 8.779 | ppl 439.35 | wps 17459 | ups 5 | wpb 3586.843 | bsz 145.540 | num_updates 1101 | lr 0.000206524 | gnorm 1.020 | clip 0.000 | oom 0.000 | wall 237 | train_wall 221
| epoch 001 | valid on 'valid' subset | loss 7.794 | nll_loss 6.855 | ppl 115.78 | num_updates 1101
| saved checkpoint /data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0/checkpoint1.pt (epoch 1 @ 1101 updates) (writing took 4.111757755279541 seconds)
Remove old epoch checkpoints
| epoch 002:     50 / 1101 loss=8.017, nll_loss=7.149, ppl=141.94, wps=18426, ups=5, wpb=3600.902, bsz=126.275, num_updates=1152, lr=0.000216086, gnorm=0.844, clip=0.000, oom=0.000, wall=254, train_wall=231
| epoch 002:    100 / 1101 loss=7.882, nll_loss=6.994, ppl=127.48, wps=19030, ups=5, wpb=3652.990, bsz=140.119, num_updates=1202, lr=0.00022546, gnorm=0.897, clip=0.000, oom=0.000, wall=264, train_wall=240
| epoch 002:    150 / 1101 loss=7.872, nll_loss=6.982, ppl=126.46, wps=18991, ups=5, wpb=3617.570, bsz=141.298, num_updates=1252, lr=0.000234834, gnorm=0.976, clip=0.000, oom=0.000, wall=273, train_wall=249
| epoch 002:    200 / 1101 loss=7.849, nll_loss=6.956, ppl=124.16, wps=19033, ups=5, wpb=3615.174, bsz=139.741, num_updates=1302, lr=0.000244209, gnorm=0.935, clip=0.000, oom=0.000, wall=282, train_wall=258
| epoch 002:    250 / 1101 loss=7.814, nll_loss=6.915, ppl=120.66, wps=19061, ups=5, wpb=3609.522, bsz=141.928, num_updates=1352, lr=0.000253583, gnorm=0.926, clip=0.000, oom=0.000, wall=292, train_wall=267
| epoch 002:    300 / 1101 loss=7.771, nll_loss=6.866, ppl=116.64, wps=18983, ups=5, wpb=3577.103, bsz=146.259, num_updates=1402, lr=0.000262957, gnorm=0.933, clip=0.000, oom=0.000, wall=301, train_wall=276
| epoch 002:    350 / 1101 loss=7.720, nll_loss=6.807, ppl=111.99, wps=19016, ups=5, wpb=3584.442, bsz=150.906, num_updates=1452, lr=0.000272332, gnorm=0.936, clip=0.000, oom=0.000, wall=310, train_wall=285
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
| epoch 002:    400 / 1101 loss=7.706, nll_loss=6.791, ppl=110.77, wps=17634, ups=5, wpb=3589.262, bsz=148.688, num_updates=1502, lr=0.000281706, gnorm=0.923, clip=0.000, oom=0.000, wall=326, train_wall=300
| epoch 002:    450 / 1101 loss=7.684, nll_loss=6.766, ppl=108.85, wps=17803, ups=5, wpb=3597.082, bsz=146.945, num_updates=1552, lr=0.000291081, gnorm=0.915, clip=0.000, oom=0.000, wall=335, train_wall=310
| epoch 002:    500 / 1101 loss=7.640, nll_loss=6.715, ppl=105.04, wps=17919, ups=5, wpb=3597.198, bsz=149.253, num_updates=1602, lr=0.000300455, gnorm=0.916, clip=0.000, oom=0.000, wall=345, train_wall=319
| epoch 002:    550 / 1101 loss=7.610, nll_loss=6.680, ppl=102.56, wps=18047, ups=5, wpb=3601.804, bsz=148.675, num_updates=1652, lr=0.000309829, gnorm=0.910, clip=0.000, oom=0.000, wall=354, train_wall=328
| epoch 002:    600 / 1101 loss=7.573, nll_loss=6.638, ppl=99.60, wps=18117, ups=5, wpb=3598.579, bsz=150.176, num_updates=1702, lr=0.000319204, gnorm=0.913, clip=0.000, oom=0.000, wall=364, train_wall=337
| epoch 002:    650 / 1101 loss=7.549, nll_loss=6.610, ppl=97.71, wps=18200, ups=5, wpb=3602.527, bsz=148.866, num_updates=1752, lr=0.000328578, gnorm=0.909, clip=0.000, oom=0.000, wall=373, train_wall=346
| epoch 002:    700 / 1101 loss=7.522, nll_loss=6.579, ppl=95.59, wps=18279, ups=5, wpb=3603.775, bsz=147.241, num_updates=1802, lr=0.000337952, gnorm=0.902, clip=0.000, oom=0.000, wall=382, train_wall=355
| epoch 002:    750 / 1101 loss=7.493, nll_loss=6.546, ppl=93.42, wps=18332, ups=5, wpb=3602.115, bsz=146.780, num_updates=1852, lr=0.000347327, gnorm=0.901, clip=0.000, oom=0.000, wall=392, train_wall=365
| epoch 002:    800 / 1101 loss=7.458, nll_loss=6.505, ppl=90.83, wps=18386, ups=5, wpb=3605.109, bsz=147.486, num_updates=1902, lr=0.000356701, gnorm=0.898, clip=0.000, oom=0.000, wall=401, train_wall=374
| epoch 002:    850 / 1101 loss=7.436, nll_loss=6.480, ppl=89.24, wps=18391, ups=5, wpb=3597.821, bsz=146.960, num_updates=1952, lr=0.000366076, gnorm=0.900, clip=0.000, oom=0.000, wall=411, train_wall=383
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
| epoch 002:    900 / 1101 loss=7.415, nll_loss=6.455, ppl=87.71, wps=17828, ups=5, wpb=3595.296, bsz=145.855, num_updates=2002, lr=0.00037545, gnorm=0.897, clip=0.000, oom=0.000, wall=426, train_wall=398
| epoch 002:    950 / 1101 loss=7.381, nll_loss=6.416, ppl=85.41, wps=17877, ups=5, wpb=3592.445, bsz=146.026, num_updates=2052, lr=0.000384824, gnorm=0.895, clip=0.000, oom=0.000, wall=435, train_wall=407
| epoch 002:   1000 / 1101 loss=7.355, nll_loss=6.386, ppl=83.64, wps=17940, ups=5, wpb=3593.697, bsz=145.302, num_updates=2102, lr=0.000394199, gnorm=0.894, clip=0.000, oom=0.000, wall=445, train_wall=416
| epoch 002:   1050 / 1101 loss=7.333, nll_loss=6.360, ppl=82.11, wps=17977, ups=5, wpb=3589.243, bsz=144.966, num_updates=2152, lr=0.000403573, gnorm=0.895, clip=0.000, oom=0.000, wall=454, train_wall=425
| epoch 002:   1100 / 1101 loss=7.301, nll_loss=6.323, ppl=80.06, wps=18010, ups=5, wpb=3586.843, bsz=145.540, num_updates=2202, lr=0.000412947, gnorm=0.895, clip=0.000, oom=0.000, wall=463, train_wall=434
| epoch 002 | loss 7.301 | nll_loss 6.323 | ppl 80.06 | wps 18003 | ups 5 | wpb 3586.843 | bsz 145.540 | num_updates 2202 | lr 0.000412947 | gnorm 0.895 | clip 0.000 | oom 0.000 | wall 464 | train_wall 434
| epoch 002 | valid on 'valid' subset | loss 6.472 | nll_loss 5.301 | ppl 39.43 | num_updates 2202 | best_loss 6.47186
| saved checkpoint /data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0/checkpoint2.pt (epoch 2 @ 2202 updates) (writing took 6.043145656585693 seconds)
Remove old epoch checkpoints
| epoch 003:     50 / 1101 loss=6.618, nll_loss=5.532, ppl=46.28, wps=18289, ups=5, wpb=3562.765, bsz=140.392, num_updates=2253, lr=0.000422509, gnorm=0.872, clip=0.000, oom=0.000, wall=483, train_wall=444
| epoch 003:    100 / 1101 loss=6.539, nll_loss=5.440, ppl=43.40, wps=18644, ups=5, wpb=3568.871, bsz=150.257, num_updates=2303, lr=0.000431884, gnorm=0.854, clip=0.000, oom=0.000, wall=492, train_wall=453
| epoch 003:    150 / 1101 loss=6.497, nll_loss=5.391, ppl=41.96, wps=18705, ups=5, wpb=3593.980, bsz=150.728, num_updates=2353, lr=0.000441258, gnorm=0.859, clip=0.000, oom=0.000, wall=502, train_wall=463
| epoch 003:    200 / 1101 loss=6.483, nll_loss=5.373, ppl=41.45, wps=18604, ups=5, wpb=3557.945, bsz=151.045, num_updates=2403, lr=0.000450632, gnorm=0.867, clip=0.000, oom=0.000, wall=511, train_wall=472
| epoch 003:    250 / 1101 loss=6.444, nll_loss=5.329, ppl=40.20, wps=18711, ups=5, wpb=3570.968, bsz=153.339, num_updates=2453, lr=0.000460007, gnorm=0.866, clip=0.000, oom=0.000, wall=520, train_wall=481
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
| epoch 003:    300 / 1101 loss=6.434, nll_loss=5.316, ppl=39.84, wps=17031, ups=5, wpb=3573.950, bsz=150.219, num_updates=2503, lr=0.000469381, gnorm=0.865, clip=0.000, oom=0.000, wall=536, train_wall=496
| epoch 003:    350 / 1101 loss=6.412, nll_loss=5.290, ppl=39.12, wps=17339, ups=5, wpb=3579.396, bsz=149.698, num_updates=2553, lr=0.000478756, gnorm=0.862, clip=0.000, oom=0.000, wall=545, train_wall=505
| epoch 003:    400 / 1101 loss=6.385, nll_loss=5.258, ppl=38.27, wps=17558, ups=5, wpb=3581.364, bsz=150.224, num_updates=2603, lr=0.00048813, gnorm=0.862, clip=0.000, oom=0.000, wall=554, train_wall=514
| epoch 003:    450 / 1101 loss=6.349, nll_loss=5.217, ppl=37.20, wps=17726, ups=5, wpb=3583.814, bsz=151.894, num_updates=2653, lr=0.000497504, gnorm=0.866, clip=0.000, oom=0.000, wall=564, train_wall=523
| epoch 003:    500 / 1101 loss=6.322, nll_loss=5.185, ppl=36.38, wps=17876, ups=5, wpb=3589.798, bsz=152.495, num_updates=2703, lr=0.000506879, gnorm=0.865, clip=0.000, oom=0.000, wall=573, train_wall=532
| epoch 003:    550 / 1101 loss=6.308, nll_loss=5.168, ppl=35.96, wps=17983, ups=5, wpb=3585.822, bsz=151.158, num_updates=2753, lr=0.000516253, gnorm=0.866, clip=0.000, oom=0.000, wall=582, train_wall=541
| epoch 003:    600 / 1101 loss=6.294, nll_loss=5.151, ppl=35.53, wps=18088, ups=5, wpb=3584.745, bsz=149.870, num_updates=2803, lr=0.000525627, gnorm=0.864, clip=0.000, oom=0.000, wall=592, train_wall=550
| epoch 003:    650 / 1101 loss=6.269, nll_loss=5.122, ppl=34.82, wps=18197, ups=5, wpb=3591.435, bsz=149.997, num_updates=2853, lr=0.000535002, gnorm=0.863, clip=0.000, oom=0.000, wall=601, train_wall=559
| epoch 003:    700 / 1101 loss=6.251, nll_loss=5.100, ppl=34.31, wps=18268, ups=5, wpb=3586.014, bsz=149.957, num_updates=2903, lr=0.000544376, gnorm=0.864, clip=0.000, oom=0.000, wall=610, train_wall=568
| epoch 003:    750 / 1101 loss=6.226, nll_loss=5.071, ppl=33.62, wps=18370, ups=5, wpb=3593.711, bsz=149.965, num_updates=2953, lr=0.000553751, gnorm=0.859, clip=0.000, oom=0.000, wall=620, train_wall=577
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
| epoch 003:    800 / 1101 loss=6.213, nll_loss=5.056, ppl=33.26, wps=17797, ups=5, wpb=3594.958, bsz=148.994, num_updates=3003, lr=0.000563125, gnorm=0.859, clip=0.000, oom=0.000, wall=634, train_wall=592
| epoch 003:    850 / 1101 loss=6.197, nll_loss=5.037, ppl=32.82, wps=17909, ups=5, wpb=3594.608, bsz=148.240, num_updates=3053, lr=0.000572499, gnorm=0.859, clip=0.000, oom=0.000, wall=643, train_wall=601
| epoch 003:    900 / 1101 loss=6.185, nll_loss=5.023, ppl=32.50, wps=17999, ups=5, wpb=3591.625, bsz=146.815, num_updates=3103, lr=0.000581874, gnorm=0.860, clip=0.000, oom=0.000, wall=652, train_wall=609
| epoch 003:    950 / 1101 loss=6.168, nll_loss=5.002, ppl=32.05, wps=18094, ups=5, wpb=3590.651, bsz=146.338, num_updates=3153, lr=0.000591248, gnorm=0.859, clip=0.000, oom=0.000, wall=661, train_wall=618
| epoch 003:   1000 / 1101 loss=6.145, nll_loss=4.976, ppl=31.46, wps=18173, ups=5, wpb=3590.734, bsz=146.572, num_updates=3203, lr=0.000600622, gnorm=0.856, clip=0.000, oom=0.000, wall=670, train_wall=627
| epoch 003:   1050 / 1101 loss=6.126, nll_loss=4.953, ppl=30.97, wps=18261, ups=5, wpb=3592.754, bsz=145.872, num_updates=3253, lr=0.000609997, gnorm=0.855, clip=0.000, oom=0.000, wall=679, train_wall=636
| epoch 003:   1100 / 1101 loss=6.109, nll_loss=4.933, ppl=30.55, wps=18309, ups=5, wpb=3586.843, bsz=145.540, num_updates=3303, lr=0.000619371, gnorm=0.855, clip=0.000, oom=0.000, wall=688, train_wall=644
| epoch 003 | loss 6.109 | nll_loss 4.933 | ppl 30.55 | wps 18301 | ups 5 | wpb 3586.843 | bsz 145.540 | num_updates 3303 | lr 0.000619371 | gnorm 0.855 | clip 0.000 | oom 0.000 | wall 688 | train_wall 644
| epoch 003 | valid on 'valid' subset | loss 5.335 | nll_loss 3.945 | ppl 15.40 | num_updates 3303 | best_loss 5.33535
| saved checkpoint /data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0/checkpoint3.pt (epoch 3 @ 3303 updates) (writing took 4.251846790313721 seconds)
Remove old epoch checkpoints
| epoch 004:     50 / 1101 loss=5.527, nll_loss=4.255, ppl=19.10, wps=18612, ups=5, wpb=3508.804, bsz=157.961, num_updates=3354, lr=0.000628933, gnorm=0.821, clip=0.000, oom=0.000, wall=705, train_wall=654
| epoch 004:    100 / 1101 loss=5.575, nll_loss=4.308, ppl=19.81, wps=18382, ups=5, wpb=3545.644, bsz=146.218, num_updates=3404, lr=0.000638307, gnorm=0.820, clip=0.000, oom=0.000, wall=715, train_wall=663
| epoch 004:    150 / 1101 loss=5.553, nll_loss=4.282, ppl=19.46, wps=18632, ups=5, wpb=3556.589, bsz=143.576, num_updates=3454, lr=0.000647682, gnorm=0.820, clip=0.000, oom=0.000, wall=724, train_wall=672
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
| epoch 004:    200 / 1101 loss=5.518, nll_loss=4.242, ppl=18.92, wps=16286, ups=5, wpb=3565.065, bsz=146.985, num_updates=3504, lr=0.000657056, gnorm=0.817, clip=0.000, oom=0.000, wall=740, train_wall=687
| epoch 004:    250 / 1101 loss=5.520, nll_loss=4.244, ppl=18.94, wps=16725, ups=5, wpb=3544.343, bsz=144.223, num_updates=3554, lr=0.000666431, gnorm=0.823, clip=0.000, oom=0.000, wall=749, train_wall=696
| epoch 004:    300 / 1101 loss=5.527, nll_loss=4.251, ppl=19.04, wps=16976, ups=5, wpb=3521.578, bsz=143.309, num_updates=3604, lr=0.000675805, gnorm=0.826, clip=0.000, oom=0.000, wall=758, train_wall=705
| epoch 004:    350 / 1101 loss=5.531, nll_loss=4.255, ppl=19.09, wps=17233, ups=5, wpb=3524.604, bsz=141.741, num_updates=3654, lr=0.000685179, gnorm=0.829, clip=0.000, oom=0.000, wall=767, train_wall=714
| epoch 004:    400 / 1101 loss=5.518, nll_loss=4.240, ppl=18.90, wps=17503, ups=5, wpb=3544.491, bsz=142.062, num_updates=3704, lr=0.000694554, gnorm=0.825, clip=0.000, oom=0.000, wall=777, train_wall=723
| epoch 004:    450 / 1101 loss=5.508, nll_loss=4.228, ppl=18.74, wps=17648, ups=5, wpb=3540.741, bsz=140.503, num_updates=3754, lr=0.000703928, gnorm=0.822, clip=0.000, oom=0.000, wall=786, train_wall=732
| epoch 004:    500 / 1101 loss=5.500, nll_loss=4.219, ppl=18.63, wps=17841, ups=5, wpb=3552.092, bsz=139.287, num_updates=3804, lr=0.000713302, gnorm=0.819, clip=0.000, oom=0.000, wall=795, train_wall=741
| epoch 004:    550 / 1101 loss=5.485, nll_loss=4.202, ppl=18.40, wps=17970, ups=5, wpb=3557.675, bsz=141.211, num_updates=3854, lr=0.000722677, gnorm=0.817, clip=0.000, oom=0.000, wall=805, train_wall=751
| epoch 004:    600 / 1101 loss=5.466, nll_loss=4.180, ppl=18.12, wps=18085, ups=5, wpb=3565.280, bsz=142.561, num_updates=3904, lr=0.000732051, gnorm=0.815, clip=0.000, oom=0.000, wall=814, train_wall=760
| epoch 004:    650 / 1101 loss=5.451, nll_loss=4.163, ppl=17.91, wps=18185, ups=5, wpb=3573.000, bsz=142.880, num_updates=3954, lr=0.000741426, gnorm=0.811, clip=0.000, oom=0.000, wall=823, train_wall=769
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
| epoch 004:    700 / 1101 loss=5.437, nll_loss=4.147, ppl=17.71, wps=17511, ups=5, wpb=3577.469, bsz=143.508, num_updates=4004, lr=0.0007508, gnorm=0.809, clip=0.000, oom=0.000, wall=839, train_wall=784
| epoch 004:    750 / 1101 loss=5.431, nll_loss=4.139, ppl=17.62, wps=17643, ups=5, wpb=3583.893, bsz=143.690, num_updates=4054, lr=0.000760174, gnorm=0.809, clip=0.000, oom=0.000, wall=848, train_wall=793
| epoch 004:    800 / 1101 loss=5.424, nll_loss=4.132, ppl=17.53, wps=17712, ups=5, wpb=3578.798, bsz=143.459, num_updates=4104, lr=0.000769549, gnorm=0.809, clip=0.000, oom=0.000, wall=857, train_wall=802
| epoch 004:    850 / 1101 loss=5.403, nll_loss=4.107, ppl=17.24, wps=17796, ups=5, wpb=3580.759, bsz=146.631, num_updates=4154, lr=0.000778923, gnorm=0.811, clip=0.000, oom=0.000, wall=867, train_wall=811
| epoch 004:    900 / 1101 loss=5.389, nll_loss=4.091, ppl=17.05, wps=17888, ups=5, wpb=3586.789, bsz=148.163, num_updates=4204, lr=0.000788297, gnorm=0.808, clip=0.000, oom=0.000, wall=876, train_wall=820
| epoch 004:    950 / 1101 loss=5.383, nll_loss=4.085, ppl=16.97, wps=17950, ups=5, wpb=3587.795, bsz=147.768, num_updates=4254, lr=0.000797672, gnorm=0.806, clip=0.000, oom=0.000, wall=886, train_wall=829
| epoch 004:   1000 / 1101 loss=5.380, nll_loss=4.081, ppl=16.92, wps=18011, ups=5, wpb=3590.043, bsz=147.020, num_updates=4304, lr=0.000807046, gnorm=0.805, clip=0.000, oom=0.000, wall=895, train_wall=839
| epoch 004:   1050 / 1101 loss=5.374, nll_loss=4.074, ppl=16.84, wps=18058, ups=5, wpb=3588.517, bsz=146.716, num_updates=4354, lr=0.000816421, gnorm=0.803, clip=0.000, oom=0.000, wall=904, train_wall=848
| epoch 004:   1100 / 1101 loss=5.375, nll_loss=4.075, ppl=16.86, wps=18103, ups=5, wpb=3586.843, bsz=145.540, num_updates=4404, lr=0.000825795, gnorm=0.802, clip=0.000, oom=0.000, wall=914, train_wall=857
| epoch 004 | loss 5.375 | nll_loss 4.075 | ppl 16.86 | wps 18096 | ups 5 | wpb 3586.843 | bsz 145.540 | num_updates 4404 | lr 0.000825795 | gnorm 0.802 | clip 0.000 | oom 0.000 | wall 914 | train_wall 857
| epoch 004 | valid on 'valid' subset | loss 4.922 | nll_loss 3.459 | ppl 11.00 | num_updates 4404 | best_loss 4.92152
| saved checkpoint /data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0/checkpoint4.pt (epoch 4 @ 4404 updates) (writing took 4.367139577865601 seconds)
Remove old epoch checkpoints
| epoch 005:     50 / 1101 loss=5.216, nll_loss=3.893, ppl=14.85, wps=19246, ups=5, wpb=3521.941, bsz=135.059, num_updates=4455, lr=0.000835357, gnorm=0.762, clip=0.000, oom=0.000, wall=930, train_wall=866
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
| epoch 005:    100 / 1101 loss=5.170, nll_loss=3.840, ppl=14.32, wps=14684, ups=4, wpb=3527.248, bsz=142.020, num_updates=4505, lr=0.000844731, gnorm=0.800, clip=0.000, oom=0.000, wall=945, train_wall=880
| epoch 005:    150 / 1101 loss=5.158, nll_loss=3.825, ppl=14.17, wps=16090, ups=5, wpb=3542.053, bsz=142.411, num_updates=4555, lr=0.000854106, gnorm=0.782, clip=0.000, oom=0.000, wall=954, train_wall=889
| epoch 005:    200 / 1101 loss=5.129, nll_loss=3.791, ppl=13.84, wps=16999, ups=5, wpb=3574.189, bsz=144.000, num_updates=4605, lr=0.00086348, gnorm=0.765, clip=0.000, oom=0.000, wall=963, train_wall=898
| epoch 005:    250 / 1101 loss=5.106, nll_loss=3.764, ppl=13.59, wps=17579, ups=5, wpb=3593.386, bsz=146.008, num_updates=4655, lr=0.000872854, gnorm=0.752, clip=0.000, oom=0.000, wall=972, train_wall=907
| epoch 005:    300 / 1101 loss=5.104, nll_loss=3.762, ppl=13.57, wps=17970, ups=5, wpb=3602.239, bsz=144.213, num_updates=4705, lr=0.000882229, gnorm=0.749, clip=0.000, oom=0.000, wall=981, train_wall=915
| epoch 005:    350 / 1101 loss=5.112, nll_loss=3.771, ppl=13.65, wps=18262, ups=5, wpb=3609.550, bsz=145.595, num_updates=4755, lr=0.000891603, gnorm=0.750, clip=0.000, oom=0.000, wall=990, train_wall=924
| epoch 005:    400 / 1101 loss=5.098, nll_loss=3.756, ppl=13.51, wps=18428, ups=5, wpb=3603.626, bsz=146.973, num_updates=4805, lr=0.000900977, gnorm=0.746, clip=0.000, oom=0.000, wall=999, train_wall=933
| epoch 005:    450 / 1101 loss=5.111, nll_loss=3.770, ppl=13.64, wps=18533, ups=5, wpb=3589.226, bsz=146.288, num_updates=4855, lr=0.000910352, gnorm=0.754, clip=0.000, oom=0.000, wall=1008, train_wall=942
| epoch 005:    500 / 1101 loss=5.111, nll_loss=3.771, ppl=13.65, wps=18661, ups=5, wpb=3588.064, bsz=146.986, num_updates=4905, lr=0.000919726, gnorm=0.750, clip=0.000, oom=0.000, wall=1017, train_wall=950
| epoch 005:    550 / 1101 loss=5.105, nll_loss=3.764, ppl=13.59, wps=18741, ups=5, wpb=3581.521, bsz=146.338, num_updates=4955, lr=0.000929101, gnorm=0.749, clip=0.000, oom=0.000, wall=1026, train_wall=959
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
| epoch 005:    600 / 1101 loss=5.097, nll_loss=3.755, ppl=13.50, wps=17915, ups=5, wpb=3584.361, bsz=147.208, num_updates=5005, lr=0.000938475, gnorm=0.744, clip=0.000, oom=0.000, wall=1041, train_wall=974
| epoch 005:    650 / 1101 loss=5.094, nll_loss=3.751, ppl=13.47, wps=18036, ups=5, wpb=3580.392, bsz=147.429, num_updates=5055, lr=0.000947849, gnorm=0.743, clip=0.000, oom=0.000, wall=1050, train_wall=983
| epoch 005:    700 / 1101 loss=5.086, nll_loss=3.743, ppl=13.39, wps=18164, ups=5, wpb=3581.304, bsz=147.367, num_updates=5105, lr=0.000957224, gnorm=0.738, clip=0.000, oom=0.000, wall=1059, train_wall=991
| epoch 005:    750 / 1101 loss=5.085, nll_loss=3.741, ppl=13.37, wps=18235, ups=5, wpb=3582.111, bsz=146.237, num_updates=5155, lr=0.000966598, gnorm=0.737, clip=0.000, oom=0.000, wall=1069, train_wall=1000
| epoch 005:    800 / 1101 loss=5.078, nll_loss=3.733, ppl=13.30, wps=18279, ups=5, wpb=3580.790, bsz=146.727, num_updates=5205, lr=0.000975972, gnorm=0.733, clip=0.000, oom=0.000, wall=1078, train_wall=1009
| epoch 005:    850 / 1101 loss=5.081, nll_loss=3.737, ppl=13.34, wps=18313, ups=5, wpb=3574.791, bsz=145.542, num_updates=5255, lr=0.000985347, gnorm=0.732, clip=0.000, oom=0.000, wall=1087, train_wall=1018
| epoch 005:    900 / 1101 loss=5.076, nll_loss=3.732, ppl=13.28, wps=18358, ups=5, wpb=3576.625, bsz=145.863, num_updates=5305, lr=0.000994721, gnorm=0.729, clip=0.000, oom=0.000, wall=1097, train_wall=1028
| epoch 005:    950 / 1101 loss=5.071, nll_loss=3.725, ppl=13.22, wps=18405, ups=5, wpb=3577.484, bsz=146.077, num_updates=5355, lr=0.0010041, gnorm=0.727, clip=0.000, oom=0.000, wall=1106, train_wall=1037
| epoch 005:   1000 / 1101 loss=5.070, nll_loss=3.724, ppl=13.21, wps=18449, ups=5, wpb=3578.226, bsz=145.438, num_updates=5405, lr=0.00101347, gnorm=0.724, clip=0.000, oom=0.000, wall=1115, train_wall=1046
| epoch 005:   1050 / 1101 loss=5.063, nll_loss=3.717, ppl=13.15, wps=18485, ups=5, wpb=3580.854, bsz=145.346, num_updates=5455, lr=0.00102284, gnorm=0.721, clip=0.000, oom=0.000, wall=1125, train_wall=1055
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
| epoch 005:   1100 / 1101 loss=5.057, nll_loss=3.710, ppl=13.09, wps=18038, ups=5, wpb=3586.843, bsz=145.540, num_updates=5505, lr=0.00103222, gnorm=0.718, clip=0.000, oom=0.000, wall=1140, train_wall=1070
| epoch 005 | loss 5.057 | nll_loss 3.710 | ppl 13.09 | wps 18031 | ups 5 | wpb 3586.843 | bsz 145.540 | num_updates 5505 | lr 0.00103222 | gnorm 0.718 | clip 0.000 | oom 0.000 | wall 1140 | train_wall 1070
| epoch 005 | valid on 'valid' subset | loss 4.743 | nll_loss 3.245 | ppl 9.48 | num_updates 5505 | best_loss 4.74283
| saved checkpoint /data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0/checkpoint5.pt (epoch 5 @ 5505 updates) (writing took 5.297482013702393 seconds)
Remove old epoch checkpoints
| epoch 006:     50 / 1101 loss=4.877, nll_loss=3.505, ppl=11.35, wps=19214, ups=5, wpb=3577.255, bsz=144.941, num_updates=5556, lr=0.00104178, gnorm=0.643, clip=0.000, oom=0.000, wall=1158, train_wall=1079
| epoch 006:    100 / 1101 loss=4.863, nll_loss=3.488, ppl=11.22, wps=19046, ups=5, wpb=3581.307, bsz=146.139, num_updates=5606, lr=0.00105115, gnorm=0.645, clip=0.000, oom=0.000, wall=1167, train_wall=1088
| epoch 006:    150 / 1101 loss=4.861, nll_loss=3.486, ppl=11.20, wps=19010, ups=5, wpb=3601.132, bsz=147.020, num_updates=5656, lr=0.00106053, gnorm=0.645, clip=0.000, oom=0.000, wall=1177, train_wall=1098
| epoch 006:    200 / 1101 loss=4.841, nll_loss=3.463, ppl=11.03, wps=18875, ups=5, wpb=3603.413, bsz=149.652, num_updates=5706, lr=0.0010699, gnorm=0.646, clip=0.000, oom=0.000, wall=1187, train_wall=1107
| epoch 006:    250 / 1101 loss=4.867, nll_loss=3.492, ppl=11.25, wps=18883, ups=5, wpb=3608.462, bsz=144.127, num_updates=5756, lr=0.00107928, gnorm=0.646, clip=0.000, oom=0.000, wall=1196, train_wall=1117
| epoch 006:    300 / 1101 loss=4.878, nll_loss=3.505, ppl=11.35, wps=18927, ups=5, wpb=3611.545, bsz=140.306, num_updates=5806, lr=0.00108865, gnorm=0.645, clip=0.000, oom=0.000, wall=1206, train_wall=1126
| epoch 006:    350 / 1101 loss=4.862, nll_loss=3.487, ppl=11.21, wps=18880, ups=5, wpb=3597.536, bsz=142.632, num_updates=5856, lr=0.00109803, gnorm=0.645, clip=0.000, oom=0.000, wall=1215, train_wall=1135
| epoch 006:    400 / 1101 loss=4.873, nll_loss=3.499, ppl=11.31, wps=18869, ups=5, wpb=3591.431, bsz=141.506, num_updates=5906, lr=0.0011074, gnorm=0.646, clip=0.000, oom=0.000, wall=1225, train_wall=1144
| epoch 006:    450 / 1101 loss=4.876, nll_loss=3.504, ppl=11.34, wps=18860, ups=5, wpb=3587.851, bsz=141.392, num_updates=5956, lr=0.00111678, gnorm=0.645, clip=0.000, oom=0.000, wall=1234, train_wall=1153
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
| epoch 006:    500 / 1101 loss=4.888, nll_loss=3.517, ppl=11.45, wps=17762, ups=5, wpb=3584.601, bsz=140.551, num_updates=6006, lr=0.00112615, gnorm=0.648, clip=0.000, oom=0.000, wall=1249, train_wall=1168
| epoch 006:    550 / 1101 loss=4.876, nll_loss=3.503, ppl=11.34, wps=17885, ups=5, wpb=3593.829, bsz=142.650, num_updates=6056, lr=0.00113552, gnorm=0.643, clip=0.000, oom=0.000, wall=1259, train_wall=1178
| epoch 006:    600 / 1101 loss=4.875, nll_loss=3.502, ppl=11.33, wps=17991, ups=5, wpb=3595.428, bsz=142.908, num_updates=6106, lr=0.0011449, gnorm=0.640, clip=0.000, oom=0.000, wall=1268, train_wall=1187
| epoch 006:    650 / 1101 loss=4.876, nll_loss=3.504, ppl=11.35, wps=18057, ups=5, wpb=3595.267, bsz=142.968, num_updates=6156, lr=0.00115427, gnorm=0.639, clip=0.000, oom=0.000, wall=1278, train_wall=1196
| epoch 006:    700 / 1101 loss=4.875, nll_loss=3.503, ppl=11.34, wps=18137, ups=5, wpb=3599.936, bsz=143.201, num_updates=6206, lr=0.00116365, gnorm=0.638, clip=0.000, oom=0.000, wall=1287, train_wall=1205
| epoch 006:    750 / 1101 loss=4.883, nll_loss=3.512, ppl=11.41, wps=18153, ups=5, wpb=3591.111, bsz=141.411, num_updates=6256, lr=0.00117302, gnorm=0.638, clip=0.000, oom=0.000, wall=1297, train_wall=1215
| epoch 006:    800 / 1101 loss=4.890, nll_loss=3.520, ppl=11.47, wps=18168, ups=5, wpb=3586.087, bsz=140.584, num_updates=6306, lr=0.0011824, gnorm=0.637, clip=0.000, oom=0.000, wall=1306, train_wall=1224
| epoch 006:    850 / 1101 loss=4.891, nll_loss=3.521, ppl=11.48, wps=18181, ups=5, wpb=3581.801, bsz=141.385, num_updates=6356, lr=0.00119177, gnorm=0.637, clip=0.000, oom=0.000, wall=1316, train_wall=1233
| epoch 006:    900 / 1101 loss=4.880, nll_loss=3.509, ppl=11.38, wps=18233, ups=5, wpb=3585.956, bsz=143.786, num_updates=6406, lr=0.00120114, gnorm=0.634, clip=0.000, oom=0.000, wall=1325, train_wall=1242
| epoch 006:    950 / 1101 loss=4.882, nll_loss=3.512, ppl=11.41, wps=18264, ups=5, wpb=3584.152, bsz=142.889, num_updates=6456, lr=0.00121052, gnorm=0.632, clip=0.000, oom=0.000, wall=1335, train_wall=1252
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
| epoch 006:   1000 / 1101 loss=4.878, nll_loss=3.508, ppl=11.37, wps=17755, ups=5, wpb=3584.055, bsz=143.631, num_updates=6506, lr=0.00121989, gnorm=0.631, clip=0.000, oom=0.000, wall=1350, train_wall=1267
| epoch 006:   1050 / 1101 loss=4.870, nll_loss=3.499, ppl=11.30, wps=17812, ups=5, wpb=3585.771, bsz=145.324, num_updates=6556, lr=0.00122927, gnorm=0.628, clip=0.000, oom=0.000, wall=1360, train_wall=1276
| epoch 006:   1100 / 1101 loss=4.867, nll_loss=3.495, ppl=11.28, wps=17858, ups=5, wpb=3586.843, bsz=145.540, num_updates=6606, lr=0.00123864, gnorm=0.626, clip=0.000, oom=0.000, wall=1369, train_wall=1285
| epoch 006 | loss 4.867 | nll_loss 3.495 | ppl 11.28 | wps 17849 | ups 5 | wpb 3586.843 | bsz 145.540 | num_updates 6606 | lr 0.00123864 | gnorm 0.626 | clip 0.000 | oom 0.000 | wall 1369 | train_wall 1285
| epoch 006 | valid on 'valid' subset | loss 4.591 | nll_loss 3.064 | ppl 8.36 | num_updates 6606 | best_loss 4.59131
| saved checkpoint /data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0/checkpoint6.pt (epoch 6 @ 6606 updates) (writing took 4.544820070266724 seconds)
Remove old epoch checkpoints
Remove /data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0/checkpoint1.pt
| epoch 007:     50 / 1101 loss=4.721, nll_loss=3.329, ppl=10.05, wps=18623, ups=5, wpb=3618.020, bsz=151.216, num_updates=6657, lr=0.0012482, gnorm=0.576, clip=0.000, oom=0.000, wall=1387, train_wall=1295
| epoch 007:    100 / 1101 loss=4.725, nll_loss=3.334, ppl=10.08, wps=18791, ups=5, wpb=3586.455, bsz=147.564, num_updates=6707, lr=0.00125758, gnorm=0.577, clip=0.000, oom=0.000, wall=1396, train_wall=1304
| epoch 007:    150 / 1101 loss=4.741, nll_loss=3.352, ppl=10.21, wps=18786, ups=5, wpb=3589.609, bsz=143.788, num_updates=6757, lr=0.00126695, gnorm=0.576, clip=0.000, oom=0.000, wall=1406, train_wall=1313
| epoch 007:    200 / 1101 loss=4.756, nll_loss=3.369, ppl=10.33, wps=18805, ups=5, wpb=3585.701, bsz=139.741, num_updates=6807, lr=0.00127633, gnorm=0.578, clip=0.000, oom=0.000, wall=1415, train_wall=1322
| epoch 007:    250 / 1101 loss=4.741, nll_loss=3.351, ppl=10.20, wps=18827, ups=5, wpb=3581.514, bsz=145.594, num_updates=6857, lr=0.0012857, gnorm=0.576, clip=0.000, oom=0.000, wall=1425, train_wall=1332
| epoch 007:    300 / 1101 loss=4.734, nll_loss=3.343, ppl=10.15, wps=18829, ups=5, wpb=3577.216, bsz=145.542, num_updates=6907, lr=0.00129508, gnorm=0.573, clip=0.000, oom=0.000, wall=1434, train_wall=1341
| epoch 007:    350 / 1101 loss=4.722, nll_loss=3.329, ppl=10.05, wps=18795, ups=5, wpb=3569.672, bsz=146.621, num_updates=6957, lr=0.00130445, gnorm=0.571, clip=0.000, oom=0.000, wall=1444, train_wall=1350
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
| epoch 007:    400 / 1101 loss=4.719, nll_loss=3.326, ppl=10.03, wps=17480, ups=5, wpb=3579.444, bsz=146.354, num_updates=7007, lr=0.00131382, gnorm=0.570, clip=0.000, oom=0.000, wall=1459, train_wall=1365
| epoch 007:    450 / 1101 loss=4.727, nll_loss=3.335, ppl=10.09, wps=17588, ups=5, wpb=3579.734, bsz=145.685, num_updates=7057, lr=0.0013232, gnorm=0.570, clip=0.000, oom=0.000, wall=1469, train_wall=1375
| epoch 007:    500 / 1101 loss=4.723, nll_loss=3.331, ppl=10.06, wps=17765, ups=5, wpb=3589.092, bsz=146.283, num_updates=7107, lr=0.00133257, gnorm=0.565, clip=0.000, oom=0.000, wall=1478, train_wall=1384
| epoch 007:    550 / 1101 loss=4.722, nll_loss=3.330, ppl=10.06, wps=17860, ups=5, wpb=3585.626, bsz=146.192, num_updates=7157, lr=0.00134195, gnorm=0.564, clip=0.000, oom=0.000, wall=1488, train_wall=1393
| epoch 007:    600 / 1101 loss=4.726, nll_loss=3.334, ppl=10.09, wps=17944, ups=5, wpb=3589.361, bsz=146.130, num_updates=7207, lr=0.00135132, gnorm=0.563, clip=0.000, oom=0.000, wall=1497, train_wall=1402
| epoch 007:    650 / 1101 loss=4.727, nll_loss=3.336, ppl=10.10, wps=17989, ups=5, wpb=3584.642, bsz=146.974, num_updates=7257, lr=0.0013607, gnorm=0.563, clip=0.000, oom=0.000, wall=1507, train_wall=1411
| epoch 007:    700 / 1101 loss=4.735, nll_loss=3.345, ppl=10.16, wps=18065, ups=5, wpb=3585.836, bsz=145.027, num_updates=7307, lr=0.00137007, gnorm=0.562, clip=0.000, oom=0.000, wall=1516, train_wall=1421
| epoch 007:    750 / 1101 loss=4.738, nll_loss=3.349, ppl=10.19, wps=18129, ups=5, wpb=3586.350, bsz=144.936, num_updates=7357, lr=0.00137945, gnorm=0.560, clip=0.000, oom=0.000, wall=1525, train_wall=1430
| epoch 007:    800 / 1101 loss=4.738, nll_loss=3.349, ppl=10.19, wps=18128, ups=5, wpb=3589.411, bsz=144.189, num_updates=7407, lr=0.00138882, gnorm=0.559, clip=0.000, oom=0.000, wall=1536, train_wall=1440
| epoch 007:    850 / 1101 loss=4.736, nll_loss=3.346, ppl=10.17, wps=18090, ups=5, wpb=3588.005, bsz=144.995, num_updates=7457, lr=0.00139819, gnorm=0.556, clip=0.000, oom=0.000, wall=1546, train_wall=1449
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
| epoch 007:    900 / 1101 loss=4.735, nll_loss=3.346, ppl=10.17, wps=17567, ups=5, wpb=3591.143, bsz=145.180, num_updates=7507, lr=0.00140757, gnorm=0.554, clip=0.000, oom=0.000, wall=1561, train_wall=1465
| epoch 007:    950 / 1101 loss=4.737, nll_loss=3.349, ppl=10.19, wps=17629, ups=5, wpb=3587.805, bsz=144.655, num_updates=7557, lr=0.00141694, gnorm=0.554, clip=0.000, oom=0.000, wall=1570, train_wall=1474
| epoch 007:   1000 / 1101 loss=4.743, nll_loss=3.355, ppl=10.23, wps=17688, ups=5, wpb=3583.019, bsz=144.295, num_updates=7607, lr=0.00142632, gnorm=0.554, clip=0.000, oom=0.000, wall=1580, train_wall=1483
| epoch 007:   1050 / 1101 loss=4.743, nll_loss=3.356, ppl=10.24, wps=17760, ups=5, wpb=3585.215, bsz=143.984, num_updates=7657, lr=0.00143569, gnorm=0.551, clip=0.000, oom=0.000, wall=1589, train_wall=1492
| epoch 007:   1100 / 1101 loss=4.740, nll_loss=3.353, ppl=10.22, wps=17825, ups=5, wpb=3586.843, bsz=145.540, num_updates=7707, lr=0.00144507, gnorm=0.551, clip=0.000, oom=0.000, wall=1598, train_wall=1501
| epoch 007 | loss 4.740 | nll_loss 3.353 | ppl 10.22 | wps 17818 | ups 5 | wpb 3586.843 | bsz 145.540 | num_updates 7707 | lr 0.00144507 | gnorm 0.551 | clip 0.000 | oom 0.000 | wall 1599 | train_wall 1501
| epoch 007 | valid on 'valid' subset | loss 4.522 | nll_loss 3.018 | ppl 8.10 | num_updates 7707 | best_loss 4.52181
| saved checkpoint /data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0/checkpoint7.pt (epoch 7 @ 7707 updates) (writing took 4.603795766830444 seconds)
Remove old epoch checkpoints
Remove /data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0/checkpoint2.pt
| epoch 008:     50 / 1101 loss=4.607, nll_loss=3.198, ppl=9.18, wps=18381, ups=5, wpb=3500.490, bsz=144.314, num_updates=7758, lr=0.00145463, gnorm=0.527, clip=0.000, oom=0.000, wall=1616, train_wall=1510
| epoch 008:    100 / 1101 loss=4.617, nll_loss=3.211, ppl=9.26, wps=18840, ups=5, wpb=3615.149, bsz=143.604, num_updates=7808, lr=0.001464, gnorm=0.518, clip=0.000, oom=0.000, wall=1625, train_wall=1520
| epoch 008:    150 / 1101 loss=4.613, nll_loss=3.206, ppl=9.23, wps=18885, ups=5, wpb=3620.212, bsz=144.901, num_updates=7858, lr=0.00147338, gnorm=0.509, clip=0.000, oom=0.000, wall=1635, train_wall=1529
| epoch 008:    200 / 1101 loss=4.574, nll_loss=3.163, ppl=8.95, wps=18952, ups=5, wpb=3635.910, bsz=153.274, num_updates=7908, lr=0.00148275, gnorm=0.501, clip=0.000, oom=0.000, wall=1645, train_wall=1538
| epoch 008:    250 / 1101 loss=4.583, nll_loss=3.173, ppl=9.02, wps=18916, ups=5, wpb=3620.657, bsz=153.753, num_updates=7958, lr=0.00149213, gnorm=0.503, clip=0.000, oom=0.000, wall=1654, train_wall=1548
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
----------> number of V layers: 6
--------------------> Save the layer stats
# of layers in model: 96, # of layers in optimizer: 0, # of layers in esd analysis: 96
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 008:    300 / 1101 loss=4.624, nll_loss=3.220, ppl=9.32, wps=17121, ups=5, wpb=3601.571, bsz=148.093, num_updates=8008, lr=0.00149925, gnorm=0.510, clip=0.000, oom=0.000, wall=1669, train_wall=1563
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 008:    350 / 1101 loss=4.638, nll_loss=3.236, ppl=9.42, wps=17309, ups=5, wpb=3601.647, bsz=144.296, num_updates=8058, lr=0.00149459, gnorm=0.511, clip=0.000, oom=0.000, wall=1679, train_wall=1572
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 008:    400 / 1101 loss=4.646, nll_loss=3.245, ppl=9.48, wps=17505, ups=5, wpb=3599.479, bsz=142.903, num_updates=8108, lr=0.00148998, gnorm=0.510, clip=0.000, oom=0.000, wall=1688, train_wall=1581
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 008:    450 / 1101 loss=4.645, nll_loss=3.244, ppl=9.47, wps=17676, ups=5, wpb=3605.098, bsz=142.333, num_updates=8158, lr=0.0014854, gnorm=0.509, clip=0.000, oom=0.000, wall=1698, train_wall=1590
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 008:    500 / 1101 loss=4.643, nll_loss=3.241, ppl=9.46, wps=17780, ups=5, wpb=3600.677, bsz=141.509, num_updates=8208, lr=0.00148087, gnorm=0.507, clip=0.000, oom=0.000, wall=1707, train_wall=1600
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 008:    550 / 1101 loss=4.639, nll_loss=3.238, ppl=9.43, wps=17880, ups=5, wpb=3600.599, bsz=142.343, num_updates=8258, lr=0.00147638, gnorm=0.506, clip=0.000, oom=0.000, wall=1717, train_wall=1609
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 008:    600 / 1101 loss=4.636, nll_loss=3.235, ppl=9.41, wps=17961, ups=5, wpb=3601.656, bsz=143.559, num_updates=8308, lr=0.00147193, gnorm=0.503, clip=0.000, oom=0.000, wall=1727, train_wall=1618
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 008:    650 / 1101 loss=4.635, nll_loss=3.233, ppl=9.40, wps=18022, ups=5, wpb=3597.685, bsz=143.618, num_updates=8358, lr=0.00146752, gnorm=0.502, clip=0.000, oom=0.000, wall=1736, train_wall=1627
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 008:    700 / 1101 loss=4.634, nll_loss=3.232, ppl=9.40, wps=18058, ups=5, wpb=3591.364, bsz=144.330, num_updates=8408, lr=0.00146315, gnorm=0.501, clip=0.000, oom=0.000, wall=1745, train_wall=1637
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 008:    750 / 1101 loss=4.627, nll_loss=3.224, ppl=9.34, wps=18123, ups=5, wpb=3594.908, bsz=145.586, num_updates=8458, lr=0.00145882, gnorm=0.499, clip=0.000, oom=0.000, wall=1755, train_wall=1646
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
----------> number of V layers: 6
--------------------> Save the layer stats
# of layers in model: 96, # of layers in optimizer: 0, # of layers in esd analysis: 96
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 008:    800 / 1101 loss=4.628, nll_loss=3.225, ppl=9.35, wps=17501, ups=5, wpb=3592.637, bsz=145.117, num_updates=8508, lr=0.00145453, gnorm=0.499, clip=0.000, oom=0.000, wall=1770, train_wall=1661
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 008:    850 / 1101 loss=4.625, nll_loss=3.222, ppl=9.33, wps=17566, ups=5, wpb=3593.002, bsz=146.105, num_updates=8558, lr=0.00145027, gnorm=0.497, clip=0.000, oom=0.000, wall=1780, train_wall=1670
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 008:    900 / 1101 loss=4.619, nll_loss=3.216, ppl=9.29, wps=17655, ups=5, wpb=3599.059, bsz=147.115, num_updates=8608, lr=0.00144606, gnorm=0.494, clip=0.000, oom=0.000, wall=1790, train_wall=1680
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 008:    950 / 1101 loss=4.617, nll_loss=3.214, ppl=9.28, wps=17710, ups=5, wpb=3596.332, bsz=147.574, num_updates=8658, lr=0.00144187, gnorm=0.492, clip=0.000, oom=0.000, wall=1799, train_wall=1689
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 008:   1000 / 1101 loss=4.621, nll_loss=3.219, ppl=9.31, wps=17745, ups=5, wpb=3591.959, bsz=146.397, num_updates=8708, lr=0.00143773, gnorm=0.491, clip=0.000, oom=0.000, wall=1809, train_wall=1698
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 008:   1050 / 1101 loss=4.621, nll_loss=3.219, ppl=9.31, wps=17786, ups=5, wpb=3589.421, bsz=146.298, num_updates=8758, lr=0.00143362, gnorm=0.490, clip=0.000, oom=0.000, wall=1818, train_wall=1707
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 008:   1100 / 1101 loss=4.622, nll_loss=3.220, ppl=9.32, wps=17825, ups=5, wpb=3586.843, bsz=145.540, num_updates=8808, lr=0.00142954, gnorm=0.489, clip=0.000, oom=0.000, wall=1828, train_wall=1717
| epoch 008 | loss 4.622 | nll_loss 3.220 | ppl 9.32 | wps 17818 | ups 5 | wpb 3586.843 | bsz 145.540 | num_updates 8808 | lr 0.00142954 | gnorm 0.489 | clip 0.000 | oom 0.000 | wall 1828 | train_wall 1717
| epoch 008 | valid on 'valid' subset | loss 4.399 | nll_loss 2.860 | ppl 7.26 | num_updates 8808 | best_loss 4.3992
----------> number of V layers: 6
| saved checkpoint /data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0/checkpoint8.pt (epoch 8 @ 8808 updates) (writing took 5.205142021179199 seconds)
Remove old epoch checkpoints
Remove /data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0/checkpoint3.pt
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 009:     50 / 1101 loss=4.461, nll_loss=3.037, ppl=8.21, wps=16522, ups=5, wpb=3629.490, bsz=155.294, num_updates=8859, lr=0.00142542, gnorm=0.459, clip=0.000, oom=0.000, wall=1847, train_wall=1727
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 009:    100 / 1101 loss=4.427, nll_loss=2.998, ppl=7.99, wps=16830, ups=5, wpb=3601.545, bsz=156.752, num_updates=8909, lr=0.00142142, gnorm=0.449, clip=0.000, oom=0.000, wall=1857, train_wall=1738
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 009:    150 / 1101 loss=4.413, nll_loss=2.982, ppl=7.90, wps=17370, ups=5, wpb=3579.013, bsz=157.192, num_updates=8959, lr=0.00141745, gnorm=0.444, clip=0.000, oom=0.000, wall=1867, train_wall=1747
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
----------> number of V layers: 6
--------------------> Save the layer stats
# of layers in model: 96, # of layers in optimizer: 0, # of layers in esd analysis: 96
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 009:    200 / 1101 loss=4.437, nll_loss=3.009, ppl=8.05, wps=15487, ups=4, wpb=3591.990, bsz=150.806, num_updates=9009, lr=0.00141351, gnorm=0.443, clip=0.000, oom=0.000, wall=1882, train_wall=1762
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 009:    250 / 1101 loss=4.425, nll_loss=2.995, ppl=7.98, wps=16025, ups=4, wpb=3596.271, bsz=154.837, num_updates=9059, lr=0.0014096, gnorm=0.441, clip=0.000, oom=0.000, wall=1892, train_wall=1772
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 009:    300 / 1101 loss=4.436, nll_loss=3.009, ppl=8.05, wps=16461, ups=5, wpb=3602.635, bsz=152.744, num_updates=9109, lr=0.00140573, gnorm=0.441, clip=0.000, oom=0.000, wall=1902, train_wall=1781
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 009:    350 / 1101 loss=4.433, nll_loss=3.005, ppl=8.03, wps=16788, ups=5, wpb=3608.419, bsz=153.231, num_updates=9159, lr=0.00140188, gnorm=0.438, clip=0.000, oom=0.000, wall=1911, train_wall=1790
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 009:    400 / 1101 loss=4.449, nll_loss=3.023, ppl=8.13, wps=16971, ups=5, wpb=3598.060, bsz=150.623, num_updates=9209, lr=0.00139807, gnorm=0.442, clip=0.000, oom=0.000, wall=1921, train_wall=1799
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 009:    450 / 1101 loss=4.451, nll_loss=3.026, ppl=8.15, wps=17064, ups=5, wpb=3587.696, bsz=149.942, num_updates=9259, lr=0.00139429, gnorm=0.444, clip=0.000, oom=0.000, wall=1931, train_wall=1809
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 009:    500 / 1101 loss=4.451, nll_loss=3.026, ppl=8.15, wps=17198, ups=5, wpb=3590.309, bsz=148.774, num_updates=9309, lr=0.00139054, gnorm=0.442, clip=0.000, oom=0.000, wall=1940, train_wall=1818
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 009:    550 / 1101 loss=4.459, nll_loss=3.035, ppl=8.20, wps=17257, ups=5, wpb=3575.947, bsz=147.151, num_updates=9359, lr=0.00138682, gnorm=0.445, clip=0.000, oom=0.000, wall=1950, train_wall=1828
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 009:    600 / 1101 loss=4.462, nll_loss=3.039, ppl=8.22, wps=17424, ups=5, wpb=3582.840, bsz=148.140, num_updates=9409, lr=0.00138313, gnorm=0.444, clip=0.000, oom=0.000, wall=1959, train_wall=1837
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 009:    650 / 1101 loss=4.464, nll_loss=3.041, ppl=8.23, wps=17529, ups=5, wpb=3581.206, bsz=148.240, num_updates=9459, lr=0.00137947, gnorm=0.444, clip=0.000, oom=0.000, wall=1969, train_wall=1846
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
----------> number of V layers: 6
--------------------> Save the layer stats
# of layers in model: 96, # of layers in optimizer: 0, # of layers in esd analysis: 96
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 009:    700 / 1101 loss=4.465, nll_loss=3.042, ppl=8.24, wps=16948, ups=5, wpb=3589.203, bsz=147.104, num_updates=9509, lr=0.00137584, gnorm=0.442, clip=0.000, oom=0.000, wall=1984, train_wall=1861
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 009:    750 / 1101 loss=4.463, nll_loss=3.040, ppl=8.22, wps=17050, ups=5, wpb=3583.531, bsz=146.727, num_updates=9559, lr=0.00137224, gnorm=0.443, clip=0.000, oom=0.000, wall=1994, train_wall=1870
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 009:    800 / 1101 loss=4.464, nll_loss=3.042, ppl=8.23, wps=17158, ups=5, wpb=3584.218, bsz=146.337, num_updates=9609, lr=0.00136866, gnorm=0.442, clip=0.000, oom=0.000, wall=2003, train_wall=1880
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 009:    850 / 1101 loss=4.464, nll_loss=3.042, ppl=8.23, wps=17263, ups=5, wpb=3586.060, bsz=146.397, num_updates=9659, lr=0.00136512, gnorm=0.441, clip=0.000, oom=0.000, wall=2013, train_wall=1889
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 009:    900 / 1101 loss=4.467, nll_loss=3.045, ppl=8.26, wps=17344, ups=5, wpb=3584.754, bsz=145.863, num_updates=9709, lr=0.0013616, gnorm=0.440, clip=0.000, oom=0.000, wall=2022, train_wall=1898
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 009:    950 / 1101 loss=4.466, nll_loss=3.044, ppl=8.25, wps=17423, ups=5, wpb=3586.973, bsz=146.094, num_updates=9759, lr=0.00135811, gnorm=0.439, clip=0.000, oom=0.000, wall=2032, train_wall=1907
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 009:   1000 / 1101 loss=4.460, nll_loss=3.038, ppl=8.21, wps=17491, ups=5, wpb=3587.573, bsz=146.620, num_updates=9809, lr=0.00135464, gnorm=0.438, clip=0.000, oom=0.000, wall=2041, train_wall=1917
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 009:   1050 / 1101 loss=4.461, nll_loss=3.039, ppl=8.22, wps=17548, ups=5, wpb=3585.118, bsz=146.039, num_updates=9859, lr=0.0013512, gnorm=0.438, clip=0.000, oom=0.000, wall=2051, train_wall=1926
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 009:   1100 / 1101 loss=4.461, nll_loss=3.038, ppl=8.21, wps=17611, ups=5, wpb=3586.843, bsz=145.540, num_updates=9909, lr=0.00134779, gnorm=0.438, clip=0.000, oom=0.000, wall=2060, train_wall=1935
| epoch 009 | loss 4.461 | nll_loss 3.038 | ppl 8.21 | wps 17604 | ups 5 | wpb 3586.843 | bsz 145.540 | num_updates 9909 | lr 0.00134779 | gnorm 0.438 | clip 0.000 | oom 0.000 | wall 2060 | train_wall 1935
| epoch 009 | valid on 'valid' subset | loss 4.286 | nll_loss 2.738 | ppl 6.67 | num_updates 9909 | best_loss 4.28606
----------> number of V layers: 6
| saved checkpoint /data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0/checkpoint9.pt (epoch 9 @ 9909 updates) (writing took 4.472686529159546 seconds)
Remove old epoch checkpoints
Remove /data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0/checkpoint4.pt
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 010:     50 / 1101 loss=4.289, nll_loss=2.845, ppl=7.18, wps=19053, ups=5, wpb=3575.824, bsz=156.235, num_updates=9960, lr=0.00134433, gnorm=0.420, clip=0.000, oom=0.000, wall=2077, train_wall=1944
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
----------> number of V layers: 6
--------------------> Save the layer stats
# of layers in model: 96, # of layers in optimizer: 0, # of layers in esd analysis: 96
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 010:    100 / 1101 loss=4.265, nll_loss=2.815, ppl=7.04, wps=14718, ups=4, wpb=3647.218, bsz=152.554, num_updates=10010, lr=0.00134097, gnorm=0.410, clip=0.000, oom=0.000, wall=2093, train_wall=1960
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 010:    150 / 1101 loss=4.302, nll_loss=2.857, ppl=7.25, wps=15869, ups=4, wpb=3617.841, bsz=147.762, num_updates=10060, lr=0.00133763, gnorm=0.414, clip=0.000, oom=0.000, wall=2102, train_wall=1969
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 010:    200 / 1101 loss=4.308, nll_loss=2.864, ppl=7.28, wps=16468, ups=5, wpb=3598.771, bsz=145.512, num_updates=10110, lr=0.00133432, gnorm=0.415, clip=0.000, oom=0.000, wall=2112, train_wall=1978
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 010:    250 / 1101 loss=4.311, nll_loss=2.867, ppl=7.29, wps=16894, ups=5, wpb=3593.873, bsz=143.777, num_updates=10160, lr=0.00133103, gnorm=0.417, clip=0.000, oom=0.000, wall=2121, train_wall=1987
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 010:    300 / 1101 loss=4.318, nll_loss=2.875, ppl=7.34, wps=17197, ups=5, wpb=3598.445, bsz=141.555, num_updates=10210, lr=0.00132777, gnorm=0.416, clip=0.000, oom=0.000, wall=2131, train_wall=1996
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 010:    350 / 1101 loss=4.312, nll_loss=2.869, ppl=7.30, wps=17433, ups=5, wpb=3594.336, bsz=144.570, num_updates=10260, lr=0.00132453, gnorm=0.417, clip=0.000, oom=0.000, wall=2140, train_wall=2006
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 010:    400 / 1101 loss=4.307, nll_loss=2.862, ppl=7.27, wps=17646, ups=5, wpb=3606.671, bsz=145.476, num_updates=10310, lr=0.00132132, gnorm=0.415, clip=0.000, oom=0.000, wall=2150, train_wall=2015
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 010:    450 / 1101 loss=4.315, nll_loss=2.872, ppl=7.32, wps=17777, ups=5, wpb=3603.184, bsz=144.497, num_updates=10360, lr=0.00131812, gnorm=0.415, clip=0.000, oom=0.000, wall=2159, train_wall=2024
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 010:    500 / 1101 loss=4.326, nll_loss=2.885, ppl=7.39, wps=17836, ups=5, wpb=3586.667, bsz=142.419, num_updates=10410, lr=0.00131495, gnorm=0.418, clip=0.000, oom=0.000, wall=2168, train_wall=2033
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 010:    550 / 1101 loss=4.327, nll_loss=2.886, ppl=7.39, wps=17918, ups=5, wpb=3584.728, bsz=142.098, num_updates=10460, lr=0.00131181, gnorm=0.417, clip=0.000, oom=0.000, wall=2178, train_wall=2042
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
----------> number of V layers: 6
--------------------> Save the layer stats
# of layers in model: 96, # of layers in optimizer: 0, # of layers in esd analysis: 96
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 010:    600 / 1101 loss=4.328, nll_loss=2.887, ppl=7.40, wps=17145, ups=5, wpb=3586.213, bsz=141.657, num_updates=10510, lr=0.00130868, gnorm=0.416, clip=0.000, oom=0.000, wall=2193, train_wall=2058
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 010:    650 / 1101 loss=4.330, nll_loss=2.889, ppl=7.41, wps=17273, ups=5, wpb=3585.773, bsz=142.525, num_updates=10560, lr=0.00130558, gnorm=0.416, clip=0.000, oom=0.000, wall=2203, train_wall=2067
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 010:    700 / 1101 loss=4.336, nll_loss=2.896, ppl=7.44, wps=17408, ups=5, wpb=3589.606, bsz=141.181, num_updates=10610, lr=0.0013025, gnorm=0.415, clip=0.000, oom=0.000, wall=2212, train_wall=2076
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 010:    750 / 1101 loss=4.341, nll_loss=2.903, ppl=7.48, wps=17487, ups=5, wpb=3585.609, bsz=142.049, num_updates=10660, lr=0.00129944, gnorm=0.416, clip=0.000, oom=0.000, wall=2222, train_wall=2085
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 010:    800 / 1101 loss=4.339, nll_loss=2.900, ppl=7.47, wps=17574, ups=5, wpb=3590.367, bsz=142.591, num_updates=10710, lr=0.00129641, gnorm=0.415, clip=0.000, oom=0.000, wall=2231, train_wall=2094
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 010:    850 / 1101 loss=4.336, nll_loss=2.897, ppl=7.45, wps=17639, ups=5, wpb=3589.522, bsz=143.989, num_updates=10760, lr=0.00129339, gnorm=0.415, clip=0.000, oom=0.000, wall=2241, train_wall=2104
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 010:    900 / 1101 loss=4.336, nll_loss=2.897, ppl=7.45, wps=17673, ups=5, wpb=3584.487, bsz=145.029, num_updates=10810, lr=0.0012904, gnorm=0.415, clip=0.000, oom=0.000, wall=2250, train_wall=2113
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 010:    950 / 1101 loss=4.336, nll_loss=2.897, ppl=7.45, wps=17747, ups=5, wpb=3587.576, bsz=145.017, num_updates=10860, lr=0.00128742, gnorm=0.415, clip=0.000, oom=0.000, wall=2260, train_wall=2122
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 010:   1000 / 1101 loss=4.335, nll_loss=2.896, ppl=7.44, wps=17795, ups=5, wpb=3585.647, bsz=145.046, num_updates=10910, lr=0.00128447, gnorm=0.414, clip=0.000, oom=0.000, wall=2269, train_wall=2131
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 010:   1050 / 1101 loss=4.333, nll_loss=2.894, ppl=7.44, wps=17839, ups=5, wpb=3584.982, bsz=145.232, num_updates=10960, lr=0.00128154, gnorm=0.413, clip=0.000, oom=0.000, wall=2279, train_wall=2141
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
----------> number of V layers: 6
--------------------> Save the layer stats
# of layers in model: 96, # of layers in optimizer: 0, # of layers in esd analysis: 96
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 010:   1100 / 1101 loss=4.333, nll_loss=2.894, ppl=7.43, wps=17424, ups=5, wpb=3586.843, bsz=145.540, num_updates=11010, lr=0.00127862, gnorm=0.413, clip=0.000, oom=0.000, wall=2294, train_wall=2156
| epoch 010 | loss 4.333 | nll_loss 2.894 | ppl 7.43 | wps 17416 | ups 5 | wpb 3586.843 | bsz 145.540 | num_updates 11010 | lr 0.00127862 | gnorm 0.413 | clip 0.000 | oom 0.000 | wall 2294 | train_wall 2156
| epoch 010 | valid on 'valid' subset | loss 4.202 | nll_loss 2.653 | ppl 6.29 | num_updates 11010 | best_loss 4.20204
----------> number of V layers: 6
| saved checkpoint /data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0/checkpoint10.pt (epoch 10 @ 11010 updates) (writing took 4.652179956436157 seconds)
Remove old epoch checkpoints
Remove /data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0/checkpoint5.pt
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 011:     50 / 1101 loss=4.202, nll_loss=2.742, ppl=6.69, wps=18449, ups=5, wpb=3628.392, bsz=138.824, num_updates=11061, lr=0.00127567, gnorm=0.393, clip=0.000, oom=0.000, wall=2312, train_wall=2166
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 011:    100 / 1101 loss=4.243, nll_loss=2.790, ppl=6.91, wps=18654, ups=5, wpb=3636.663, bsz=130.931, num_updates=11111, lr=0.0012728, gnorm=0.391, clip=0.000, oom=0.000, wall=2322, train_wall=2175
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 011:    150 / 1101 loss=4.212, nll_loss=2.754, ppl=6.75, wps=18676, ups=5, wpb=3640.106, bsz=139.921, num_updates=11161, lr=0.00126994, gnorm=0.389, clip=0.000, oom=0.000, wall=2331, train_wall=2185
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 011:    200 / 1101 loss=4.196, nll_loss=2.736, ppl=6.66, wps=18672, ups=5, wpb=3636.129, bsz=145.791, num_updates=11211, lr=0.00126711, gnorm=0.390, clip=0.000, oom=0.000, wall=2341, train_wall=2194
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 011:    250 / 1101 loss=4.196, nll_loss=2.737, ppl=6.67, wps=18773, ups=5, wpb=3647.004, bsz=148.048, num_updates=11261, lr=0.00126429, gnorm=0.388, clip=0.000, oom=0.000, wall=2351, train_wall=2203
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 011:    300 / 1101 loss=4.207, nll_loss=2.750, ppl=6.73, wps=18748, ups=5, wpb=3629.349, bsz=146.286, num_updates=11311, lr=0.0012615, gnorm=0.392, clip=0.000, oom=0.000, wall=2360, train_wall=2213
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 011:    350 / 1101 loss=4.211, nll_loss=2.754, ppl=6.75, wps=18721, ups=5, wpb=3613.607, bsz=144.365, num_updates=11361, lr=0.00125872, gnorm=0.394, clip=0.000, oom=0.000, wall=2370, train_wall=2222
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 011:    400 / 1101 loss=4.223, nll_loss=2.768, ppl=6.81, wps=18672, ups=5, wpb=3586.010, bsz=142.643, num_updates=11411, lr=0.00125596, gnorm=0.399, clip=0.000, oom=0.000, wall=2379, train_wall=2231
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 011:    450 / 1101 loss=4.225, nll_loss=2.770, ppl=6.82, wps=18712, ups=5, wpb=3590.404, bsz=143.450, num_updates=11461, lr=0.00125321, gnorm=0.399, clip=0.000, oom=0.000, wall=2388, train_wall=2240
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
----------> number of V layers: 6
--------------------> Save the layer stats
# of layers in model: 96, # of layers in optimizer: 0, # of layers in esd analysis: 96
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 011:    500 / 1101 loss=4.229, nll_loss=2.775, ppl=6.85, wps=17653, ups=5, wpb=3591.952, bsz=143.250, num_updates=11511, lr=0.00125049, gnorm=0.399, clip=0.000, oom=0.000, wall=2404, train_wall=2255
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 011:    550 / 1101 loss=4.230, nll_loss=2.776, ppl=6.85, wps=17763, ups=5, wpb=3590.675, bsz=143.187, num_updates=11561, lr=0.00124778, gnorm=0.398, clip=0.000, oom=0.000, wall=2413, train_wall=2264
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 011:    600 / 1101 loss=4.230, nll_loss=2.776, ppl=6.85, wps=17827, ups=5, wpb=3584.251, bsz=144.266, num_updates=11611, lr=0.00124509, gnorm=0.398, clip=0.000, oom=0.000, wall=2423, train_wall=2274
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 011:    650 / 1101 loss=4.234, nll_loss=2.780, ppl=6.87, wps=17877, ups=5, wpb=3581.180, bsz=144.429, num_updates=11661, lr=0.00124242, gnorm=0.399, clip=0.000, oom=0.000, wall=2432, train_wall=2283
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 011:    700 / 1101 loss=4.233, nll_loss=2.780, ppl=6.87, wps=17952, ups=5, wpb=3582.743, bsz=144.672, num_updates=11711, lr=0.00123976, gnorm=0.400, clip=0.000, oom=0.000, wall=2442, train_wall=2292
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 011:    750 / 1101 loss=4.232, nll_loss=2.779, ppl=6.86, wps=18034, ups=5, wpb=3588.232, bsz=145.607, num_updates=11761, lr=0.00123713, gnorm=0.399, clip=0.000, oom=0.000, wall=2451, train_wall=2301
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 011:    800 / 1101 loss=4.233, nll_loss=2.781, ppl=6.87, wps=18096, ups=5, wpb=3591.737, bsz=145.487, num_updates=11811, lr=0.00123451, gnorm=0.399, clip=0.000, oom=0.000, wall=2461, train_wall=2311
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 011:    850 / 1101 loss=4.235, nll_loss=2.782, ppl=6.88, wps=18143, ups=5, wpb=3593.479, bsz=144.929, num_updates=11861, lr=0.0012319, gnorm=0.399, clip=0.000, oom=0.000, wall=2470, train_wall=2320
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 011:    900 / 1101 loss=4.235, nll_loss=2.783, ppl=6.88, wps=18173, ups=5, wpb=3590.943, bsz=144.807, num_updates=11911, lr=0.00122931, gnorm=0.399, clip=0.000, oom=0.000, wall=2480, train_wall=2329
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 011:    950 / 1101 loss=4.232, nll_loss=2.779, ppl=6.86, wps=18223, ups=5, wpb=3594.501, bsz=145.311, num_updates=11961, lr=0.00122674, gnorm=0.399, clip=0.000, oom=0.000, wall=2489, train_wall=2338
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
----------> number of V layers: 6
--------------------> Save the layer stats
# of layers in model: 96, # of layers in optimizer: 0, # of layers in esd analysis: 96
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 011:   1000 / 1101 loss=4.232, nll_loss=2.780, ppl=6.87, wps=17707, ups=5, wpb=3592.041, bsz=145.358, num_updates=12011, lr=0.00122418, gnorm=0.398, clip=0.000, oom=0.000, wall=2505, train_wall=2354
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 011:   1050 / 1101 loss=4.236, nll_loss=2.784, ppl=6.89, wps=17767, ups=5, wpb=3590.521, bsz=144.798, num_updates=12061, lr=0.00122164, gnorm=0.399, clip=0.000, oom=0.000, wall=2514, train_wall=2363
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 011:   1100 / 1101 loss=4.236, nll_loss=2.784, ppl=6.89, wps=17806, ups=5, wpb=3586.843, bsz=145.540, num_updates=12111, lr=0.00121912, gnorm=0.399, clip=0.000, oom=0.000, wall=2524, train_wall=2372
| epoch 011 | loss 4.236 | nll_loss 2.784 | ppl 6.89 | wps 17799 | ups 5 | wpb 3586.843 | bsz 145.540 | num_updates 12111 | lr 0.00121912 | gnorm 0.399 | clip 0.000 | oom 0.000 | wall 2524 | train_wall 2372
| epoch 011 | valid on 'valid' subset | loss 4.178 | nll_loss 2.595 | ppl 6.04 | num_updates 12111 | best_loss 4.17805
----------> number of V layers: 6
| saved checkpoint /data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0/checkpoint11.pt (epoch 11 @ 12111 updates) (writing took 4.831558704376221 seconds)
Remove old epoch checkpoints
Remove /data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0/checkpoint6.pt
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 012:     50 / 1101 loss=4.179, nll_loss=2.716, ppl=6.57, wps=16636, ups=5, wpb=3471.235, bsz=122.510, num_updates=12162, lr=0.00121656, gnorm=0.402, clip=0.000, oom=0.000, wall=2542, train_wall=2382
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 012:    100 / 1101 loss=4.164, nll_loss=2.698, ppl=6.49, wps=16828, ups=5, wpb=3467.703, bsz=126.495, num_updates=12212, lr=0.00121407, gnorm=0.397, clip=0.000, oom=0.000, wall=2552, train_wall=2392
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 012:    150 / 1101 loss=4.144, nll_loss=2.676, ppl=6.39, wps=17413, ups=5, wpb=3506.987, bsz=131.974, num_updates=12262, lr=0.00121159, gnorm=0.390, clip=0.000, oom=0.000, wall=2562, train_wall=2401
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 012:    200 / 1101 loss=4.125, nll_loss=2.654, ppl=6.30, wps=17830, ups=5, wpb=3545.468, bsz=141.174, num_updates=12312, lr=0.00120913, gnorm=0.387, clip=0.000, oom=0.000, wall=2571, train_wall=2411
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 012:    250 / 1101 loss=4.114, nll_loss=2.643, ppl=6.25, wps=18071, ups=5, wpb=3567.661, bsz=146.550, num_updates=12362, lr=0.00120668, gnorm=0.386, clip=0.000, oom=0.000, wall=2581, train_wall=2420
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 012:    300 / 1101 loss=4.116, nll_loss=2.646, ppl=6.26, wps=18188, ups=5, wpb=3575.535, bsz=147.508, num_updates=12412, lr=0.00120425, gnorm=0.386, clip=0.000, oom=0.000, wall=2591, train_wall=2429
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 012:    350 / 1101 loss=4.116, nll_loss=2.645, ppl=6.26, wps=18280, ups=5, wpb=3589.071, bsz=148.672, num_updates=12462, lr=0.00120183, gnorm=0.385, clip=0.000, oom=0.000, wall=2600, train_wall=2439
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
----------> number of V layers: 6
--------------------> Save the layer stats
# of layers in model: 96, # of layers in optimizer: 0, # of layers in esd analysis: 96
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 012:    400 / 1101 loss=4.120, nll_loss=2.651, ppl=6.28, wps=17036, ups=5, wpb=3578.589, bsz=148.349, num_updates=12512, lr=0.00119942, gnorm=0.387, clip=0.000, oom=0.000, wall=2616, train_wall=2454
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 012:    450 / 1101 loss=4.124, nll_loss=2.655, ppl=6.30, wps=17174, ups=5, wpb=3570.020, bsz=147.548, num_updates=12562, lr=0.00119704, gnorm=0.389, clip=0.000, oom=0.000, wall=2625, train_wall=2463
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 012:    500 / 1101 loss=4.132, nll_loss=2.665, ppl=6.34, wps=17348, ups=5, wpb=3577.453, bsz=146.443, num_updates=12612, lr=0.00119466, gnorm=0.389, clip=0.000, oom=0.000, wall=2635, train_wall=2472
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 012:    550 / 1101 loss=4.141, nll_loss=2.675, ppl=6.39, wps=17467, ups=5, wpb=3573.922, bsz=144.407, num_updates=12662, lr=0.0011923, gnorm=0.391, clip=0.000, oom=0.000, wall=2644, train_wall=2482
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 012:    600 / 1101 loss=4.140, nll_loss=2.673, ppl=6.38, wps=17582, ups=5, wpb=3579.672, bsz=146.077, num_updates=12712, lr=0.00118995, gnorm=0.390, clip=0.000, oom=0.000, wall=2654, train_wall=2491
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 012:    650 / 1101 loss=4.138, nll_loss=2.671, ppl=6.37, wps=17669, ups=5, wpb=3584.429, bsz=146.962, num_updates=12762, lr=0.00118762, gnorm=0.390, clip=0.000, oom=0.000, wall=2664, train_wall=2500
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 012:    700 / 1101 loss=4.142, nll_loss=2.676, ppl=6.39, wps=17771, ups=5, wpb=3586.676, bsz=146.088, num_updates=12812, lr=0.0011853, gnorm=0.389, clip=0.000, oom=0.000, wall=2673, train_wall=2510
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 012:    750 / 1101 loss=4.141, nll_loss=2.676, ppl=6.39, wps=17839, ups=5, wpb=3587.630, bsz=146.663, num_updates=12862, lr=0.00118299, gnorm=0.390, clip=0.000, oom=0.000, wall=2683, train_wall=2519
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 012:    800 / 1101 loss=4.149, nll_loss=2.684, ppl=6.43, wps=17882, ups=5, wpb=3587.181, bsz=144.739, num_updates=12912, lr=0.0011807, gnorm=0.391, clip=0.000, oom=0.000, wall=2692, train_wall=2528
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 012:    850 / 1101 loss=4.150, nll_loss=2.686, ppl=6.43, wps=17959, ups=5, wpb=3590.012, bsz=144.996, num_updates=12962, lr=0.00117842, gnorm=0.390, clip=0.000, oom=0.000, wall=2702, train_wall=2537
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
----------> number of V layers: 6
--------------------> Save the layer stats
# of layers in model: 96, # of layers in optimizer: 0, # of layers in esd analysis: 96
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 012:    900 / 1101 loss=4.156, nll_loss=2.693, ppl=6.47, wps=17404, ups=5, wpb=3588.754, bsz=144.284, num_updates=13012, lr=0.00117615, gnorm=0.391, clip=0.000, oom=0.000, wall=2717, train_wall=2553
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 012:    950 / 1101 loss=4.157, nll_loss=2.693, ppl=6.47, wps=17413, ups=5, wpb=3587.975, bsz=144.278, num_updates=13062, lr=0.0011739, gnorm=0.391, clip=0.000, oom=0.000, wall=2727, train_wall=2563
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 012:   1000 / 1101 loss=4.156, nll_loss=2.694, ppl=6.47, wps=17362, ups=5, wpb=3590.802, bsz=144.328, num_updates=13112, lr=0.00117166, gnorm=0.390, clip=0.000, oom=0.000, wall=2738, train_wall=2573
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 012:   1050 / 1101 loss=4.153, nll_loss=2.689, ppl=6.45, wps=17313, ups=5, wpb=3591.735, bsz=146.048, num_updates=13162, lr=0.00116943, gnorm=0.389, clip=0.000, oom=0.000, wall=2750, train_wall=2584
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 012:   1100 / 1101 loss=4.155, nll_loss=2.693, ppl=6.47, wps=17245, ups=5, wpb=3586.843, bsz=145.540, num_updates=13212, lr=0.00116722, gnorm=0.390, clip=0.000, oom=0.000, wall=2760, train_wall=2595
| epoch 012 | loss 4.155 | nll_loss 2.693 | ppl 6.47 | wps 17238 | ups 5 | wpb 3586.843 | bsz 145.540 | num_updates 13212 | lr 0.00116722 | gnorm 0.390 | clip 0.000 | oom 0.000 | wall 2761 | train_wall 2595
| epoch 012 | valid on 'valid' subset | loss 4.120 | nll_loss 2.546 | ppl 5.84 | num_updates 13212 | best_loss 4.12011
----------> number of V layers: 6
| saved checkpoint /data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0/checkpoint12.pt (epoch 12 @ 13212 updates) (writing took 5.959936618804932 seconds)
Remove old epoch checkpoints
Remove /data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0/checkpoint7.pt
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 013:     50 / 1101 loss=4.055, nll_loss=2.573, ppl=5.95, wps=18663, ups=5, wpb=3465.627, bsz=136.157, num_updates=13263, lr=0.00116497, gnorm=0.383, clip=0.000, oom=0.000, wall=2779, train_wall=2604
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 013:    100 / 1101 loss=4.030, nll_loss=2.547, ppl=5.85, wps=18257, ups=5, wpb=3502.802, bsz=150.168, num_updates=13313, lr=0.00116278, gnorm=0.381, clip=0.000, oom=0.000, wall=2789, train_wall=2614
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 013:    150 / 1101 loss=4.022, nll_loss=2.538, ppl=5.81, wps=18384, ups=5, wpb=3529.252, bsz=148.073, num_updates=13363, lr=0.0011606, gnorm=0.381, clip=0.000, oom=0.000, wall=2798, train_wall=2623
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 013:    200 / 1101 loss=4.038, nll_loss=2.556, ppl=5.88, wps=18501, ups=5, wpb=3544.109, bsz=144.632, num_updates=13413, lr=0.00115844, gnorm=0.381, clip=0.000, oom=0.000, wall=2808, train_wall=2632
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 013:    250 / 1101 loss=4.039, nll_loss=2.558, ppl=5.89, wps=18523, ups=5, wpb=3546.633, bsz=144.665, num_updates=13463, lr=0.00115629, gnorm=0.384, clip=0.000, oom=0.000, wall=2817, train_wall=2642
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
----------> number of V layers: 6
--------------------> Save the layer stats
# of layers in model: 96, # of layers in optimizer: 0, # of layers in esd analysis: 96
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 013:    300 / 1101 loss=4.041, nll_loss=2.560, ppl=5.90, wps=16829, ups=5, wpb=3559.189, bsz=143.890, num_updates=13513, lr=0.00115414, gnorm=0.382, clip=0.000, oom=0.000, wall=2833, train_wall=2657
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 013:    350 / 1101 loss=4.045, nll_loss=2.565, ppl=5.92, wps=17125, ups=5, wpb=3569.256, bsz=145.251, num_updates=13563, lr=0.00115202, gnorm=0.382, clip=0.000, oom=0.000, wall=2843, train_wall=2666
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 013:    400 / 1101 loss=4.044, nll_loss=2.564, ppl=5.91, wps=17354, ups=5, wpb=3580.960, bsz=146.212, num_updates=13613, lr=0.0011499, gnorm=0.382, clip=0.000, oom=0.000, wall=2852, train_wall=2675
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 013:    450 / 1101 loss=4.059, nll_loss=2.581, ppl=5.98, wps=17506, ups=5, wpb=3578.106, bsz=143.767, num_updates=13663, lr=0.00114779, gnorm=0.384, clip=0.000, oom=0.000, wall=2862, train_wall=2685
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 013:    500 / 1101 loss=4.069, nll_loss=2.593, ppl=6.03, wps=17635, ups=5, wpb=3577.786, bsz=141.363, num_updates=13713, lr=0.0011457, gnorm=0.385, clip=0.000, oom=0.000, wall=2871, train_wall=2694
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 013:    550 / 1101 loss=4.066, nll_loss=2.589, ppl=6.02, wps=17734, ups=5, wpb=3574.949, bsz=142.633, num_updates=13763, lr=0.00114361, gnorm=0.385, clip=0.000, oom=0.000, wall=2880, train_wall=2703
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 013:    600 / 1101 loss=4.068, nll_loss=2.592, ppl=6.03, wps=17833, ups=5, wpb=3578.170, bsz=141.722, num_updates=13813, lr=0.00114154, gnorm=0.385, clip=0.000, oom=0.000, wall=2890, train_wall=2712
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 013:    650 / 1101 loss=4.064, nll_loss=2.588, ppl=6.01, wps=17944, ups=5, wpb=3590.052, bsz=143.605, num_updates=13863, lr=0.00113948, gnorm=0.383, clip=0.000, oom=0.000, wall=2900, train_wall=2722
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 013:    700 / 1101 loss=4.068, nll_loss=2.592, ppl=6.03, wps=17998, ups=5, wpb=3588.877, bsz=143.622, num_updates=13913, lr=0.00113743, gnorm=0.383, clip=0.000, oom=0.000, wall=2909, train_wall=2731
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 013:    750 / 1101 loss=4.067, nll_loss=2.591, ppl=6.03, wps=18066, ups=5, wpb=3592.868, bsz=144.606, num_updates=13963, lr=0.00113539, gnorm=0.383, clip=0.000, oom=0.000, wall=2919, train_wall=2740
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
----------> number of V layers: 6
--------------------> Save the layer stats
# of layers in model: 96, # of layers in optimizer: 0, # of layers in esd analysis: 96
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 013:    800 / 1101 loss=4.074, nll_loss=2.599, ppl=6.06, wps=17447, ups=5, wpb=3585.928, bsz=144.688, num_updates=14013, lr=0.00113337, gnorm=0.385, clip=0.000, oom=0.000, wall=2934, train_wall=2755
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 013:    850 / 1101 loss=4.076, nll_loss=2.602, ppl=6.07, wps=17567, ups=5, wpb=3595.329, bsz=145.193, num_updates=14063, lr=0.00113135, gnorm=0.384, clip=0.000, oom=0.000, wall=2944, train_wall=2765
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 013:    900 / 1101 loss=4.075, nll_loss=2.601, ppl=6.07, wps=17643, ups=5, wpb=3596.363, bsz=146.272, num_updates=14113, lr=0.00112934, gnorm=0.383, clip=0.000, oom=0.000, wall=2953, train_wall=2774
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 013:    950 / 1101 loss=4.079, nll_loss=2.606, ppl=6.09, wps=17699, ups=5, wpb=3595.697, bsz=146.237, num_updates=14163, lr=0.00112735, gnorm=0.384, clip=0.000, oom=0.000, wall=2963, train_wall=2783
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 013:   1000 / 1101 loss=4.083, nll_loss=2.610, ppl=6.11, wps=17739, ups=5, wpb=3592.373, bsz=146.109, num_updates=14213, lr=0.00112536, gnorm=0.386, clip=0.000, oom=0.000, wall=2972, train_wall=2792
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 013:   1050 / 1101 loss=4.087, nll_loss=2.615, ppl=6.13, wps=17786, ups=5, wpb=3590.088, bsz=145.438, num_updates=14263, lr=0.00112339, gnorm=0.386, clip=0.000, oom=0.000, wall=2982, train_wall=2802
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 013:   1100 / 1101 loss=4.089, nll_loss=2.617, ppl=6.14, wps=17817, ups=5, wpb=3586.843, bsz=145.540, num_updates=14313, lr=0.00112143, gnorm=0.386, clip=0.000, oom=0.000, wall=2991, train_wall=2811
| epoch 013 | loss 4.089 | nll_loss 2.617 | ppl 6.14 | wps 17810 | ups 5 | wpb 3586.843 | bsz 145.540 | num_updates 14313 | lr 0.00112143 | gnorm 0.386 | clip 0.000 | oom 0.000 | wall 2991 | train_wall 2811
| epoch 013 | valid on 'valid' subset | loss 4.061 | nll_loss 2.486 | ppl 5.60 | num_updates 14313 | best_loss 4.06115
----------> number of V layers: 6
| saved checkpoint /data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0/checkpoint13.pt (epoch 13 @ 14313 updates) (writing took 4.489551305770874 seconds)
Remove old epoch checkpoints
Remove /data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0/checkpoint8.pt
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 014:     50 / 1101 loss=4.008, nll_loss=2.521, ppl=5.74, wps=18085, ups=5, wpb=3598.196, bsz=128.314, num_updates=14364, lr=0.00111943, gnorm=0.369, clip=0.000, oom=0.000, wall=3009, train_wall=2821
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
| epoch 014:    100 / 1101 loss=3.975, nll_loss=2.485, ppl=5.60, wps=18252, ups=5, wpb=3559.356, bsz=140.990, num_updates=14414, lr=0.00111749, gnorm=0.371, clip=0.000, oom=0.000, wall=3018, train_wall=2830
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
----------> number of V layers: 6
Namespace(acc_block='True', acc_mid='False', acc_v='False', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer_iwslt_de_en_v2', attention_dropout=0.1, batchnorm='True', best_checkpoint_metric='loss', bpe=None, bucket_cap_mb=25, clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/yefan0726/data/nlp/mt/data-bin/iwslt14.tokenized.de-en.joined', dataset_impl=None, ddp_backend='no_c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_norm_ff='layer', decoder_norm_self='layer', decoder_normalize_before=True, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, dropout_gama=0.5, dropout_type='none', early_stop=10000, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_norm_ff='layer', encoder_norm_self='layer', encoder_normalize_before=True, encoder_spec_norm=False, esd_interval=500, fast_stat_sync=False, filter_zeros='False', find_unused_parameters=False, fix_batches_to_gpus=False, fix_fingers='xmin_mid', fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=5, label_smoothing=0.1, layer_wise_attention=False, lazy_load=False, left_pad_source='True', left_pad_target='False', load_alignments=False, log_format=None, log_interval=50, lr=[0.0015], lr_min_ratio=0.6, lr_scale=2.0, lr_scheduler='inverse_sqrt', lr_slope=0.8, max_epoch=55, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, metric='alpha', min_loss_scale=0.0001, min_lr=1e-09, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_token_positional_embeddings=False, num_workers=1, optimizer='adam', optimizer_overrides='{}', raw_text=False, relu_dropout=0.1, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='/data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0/checkpoint_best.pt', save_dir='/data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0', save_interval=1, save_interval_updates=0, seed=43, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, single_gpu=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tb='False', tbmf_wrapper=False, tbr_after_warm='True', temp_balance_lr='tb_linear_map', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', update_freq=[1], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_init_lr=1e-07, warmup_updates=8000, weight_decay=0.0001, xmin_pos=2.0)
| [de] dictionary: 10152 types
| [en] dictionary: 10152 types
| loaded 7283 examples from: /data/yefan0726/data/nlp/mt/data-bin/iwslt14.tokenized.de-en.joined/valid.de-en.de
| loaded 7283 examples from: /data/yefan0726/data/nlp/mt/data-bin/iwslt14.tokenized.de-en.joined/valid.de-en.en
| /data/yefan0726/data/nlp/mt/data-bin/iwslt14.tokenized.de-en.joined valid de-en 7283 examples
| model transformer_iwslt_de_en_v2, criterion LabelSmoothedCrossEntropyCriterion
| num. model params: 36743168 (num. trained: 36743168)
---------------------> Trainer: save_dir /data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0
| training on 1 GPUs
| max tokens per GPU = 4096 and max sentences per GPU = None
-------------------->file checkpoint utils load checkpoint
--------------------> Trainer.py: load_checkpoint
--------------------> bexists True
--------------------> Function: load_checkpoint, self._build_optimizer()
--------------------> _build_optimizer
Baseline INIT
| NOTICE: your device may support faster training with --fp16
#############Initialize this optimizer!!!!!
--------------------> Initialize Adam Optimizer
We should initialize the learning rate scheduler immediately
--------------------> Initialize the InverseSquareRootSchedule
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
Scheduling with block acceleration
----------> number of V layers: 6
| loaded checkpoint /data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0/checkpoint_best.pt (epoch 13 @ 14313 updates)
Scheduling with block acceleration
----------> number of V layers: 6
| loading train data for epoch 13
| loaded 160239 examples from: /data/yefan0726/data/nlp/mt/data-bin/iwslt14.tokenized.de-en.joined/train.de-en.de
| loaded 160239 examples from: /data/yefan0726/data/nlp/mt/data-bin/iwslt14.tokenized.de-en.joined/train.de-en.en
| /data/yefan0726/data/nlp/mt/data-bin/iwslt14.tokenized.de-en.joined train de-en 160239 examples
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
| epoch 014:     50 / 1101 loss=6.099, nll_loss=4.909, ppl=30.04, wps=16027, ups=4, wpb=3598.196, bsz=128.314, num_updates=14364, lr=0.00999999, gnorm=0.896, clip=0.000, oom=0.000, wall=16, train_wall=2823
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
| epoch 014:    100 / 1101 loss=6.317, nll_loss=5.161, ppl=35.78, wps=16764, ups=5, wpb=3559.356, bsz=140.990, num_updates=14414, lr=0.00999999, gnorm=0.827, clip=0.000, oom=0.000, wall=26, train_wall=2833
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
| epoch 014:    150 / 1101 loss=6.323, nll_loss=5.170, ppl=36.00, wps=17285, ups=5, wpb=3544.974, bsz=146.808, num_updates=14464, lr=0.00999999, gnorm=0.754, clip=0.000, oom=0.000, wall=35, train_wall=2842
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
Scheduling with block acceleration
----------> number of V layers: 6
--------------------> Save the layer stats
# of layers in model: 96, # of layers in optimizer: 0, # of layers in esd analysis: 96
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
| epoch 014:    200 / 1101 loss=6.300, nll_loss=5.144, ppl=35.35, wps=15469, ups=4, wpb=3575.652, bsz=146.627, num_updates=14514, lr=0.00999999, gnorm=0.725, clip=0.000, oom=0.000, wall=51, train_wall=2857
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
| epoch 014:    250 / 1101 loss=6.309, nll_loss=5.155, ppl=35.63, wps=16126, ups=4, wpb=3584.809, bsz=144.223, num_updates=14564, lr=0.00999999, gnorm=0.707, clip=0.000, oom=0.000, wall=60, train_wall=2867
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
| epoch 014:    300 / 1101 loss=6.296, nll_loss=5.140, ppl=35.26, wps=16581, ups=5, wpb=3591.329, bsz=145.568, num_updates=14614, lr=0.00999999, gnorm=0.698, clip=0.000, oom=0.000, wall=69, train_wall=2876
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
| epoch 014:    350 / 1101 loss=6.305, nll_loss=5.151, ppl=35.53, wps=16938, ups=5, wpb=3592.276, bsz=142.952, num_updates=14664, lr=0.00999999, gnorm=0.697, clip=0.000, oom=0.000, wall=79, train_wall=2885
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
| epoch 014:    400 / 1101 loss=6.300, nll_loss=5.145, ppl=35.39, wps=17190, ups=5, wpb=3589.481, bsz=142.484, num_updates=14714, lr=0.00999999, gnorm=0.699, clip=0.000, oom=0.000, wall=88, train_wall=2894
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
| epoch 014:    450 / 1101 loss=6.287, nll_loss=5.130, ppl=35.02, wps=17410, ups=5, wpb=3594.723, bsz=142.368, num_updates=14764, lr=0.00999999, gnorm=0.700, clip=0.000, oom=0.000, wall=97, train_wall=2903
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
| epoch 014:    500 / 1101 loss=6.271, nll_loss=5.113, ppl=34.60, wps=17562, ups=5, wpb=3591.814, bsz=143.473, num_updates=14814, lr=0.00999999, gnorm=0.705, clip=0.000, oom=0.000, wall=107, train_wall=2912
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
| epoch 014:    550 / 1101 loss=6.251, nll_loss=5.090, ppl=34.05, wps=17679, ups=5, wpb=3591.575, bsz=143.927, num_updates=14864, lr=0.00999999, gnorm=0.704, clip=0.000, oom=0.000, wall=116, train_wall=2921
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
| epoch 014:    600 / 1101 loss=6.236, nll_loss=5.072, ppl=33.65, wps=17731, ups=5, wpb=3588.265, bsz=144.639, num_updates=14914, lr=0.00999999, gnorm=0.707, clip=0.000, oom=0.000, wall=126, train_wall=2931
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
| epoch 014:    650 / 1101 loss=6.228, nll_loss=5.064, ppl=33.44, wps=17824, ups=5, wpb=3587.822, bsz=145.561, num_updates=14964, lr=0.00999999, gnorm=0.711, clip=0.000, oom=0.000, wall=135, train_wall=2940
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
Scheduling with block acceleration
----------> number of V layers: 6
--------------------> Save the layer stats
# of layers in model: 96, # of layers in optimizer: 0, # of layers in esd analysis: 96
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
| epoch 014:    700 / 1101 loss=6.225, nll_loss=5.061, ppl=33.38, wps=17171, ups=5, wpb=3586.629, bsz=145.198, num_updates=15014, lr=0.00999999, gnorm=0.714, clip=0.000, oom=0.000, wall=151, train_wall=2955
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
| epoch 014:    750 / 1101 loss=6.221, nll_loss=5.056, ppl=33.27, wps=17293, ups=5, wpb=3587.668, bsz=144.501, num_updates=15064, lr=0.00999999, gnorm=0.719, clip=0.000, oom=0.000, wall=160, train_wall=2964
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
| epoch 014:    800 / 1101 loss=6.214, nll_loss=5.048, ppl=33.08, wps=17408, ups=5, wpb=3590.513, bsz=143.670, num_updates=15114, lr=0.00999999, gnorm=0.720, clip=0.000, oom=0.000, wall=169, train_wall=2973
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
| epoch 014:    850 / 1101 loss=6.203, nll_loss=5.036, ppl=32.80, wps=17477, ups=5, wpb=3588.014, bsz=144.320, num_updates=15164, lr=0.00999999, gnorm=0.725, clip=0.000, oom=0.000, wall=179, train_wall=2982
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Namespace(acc_block='True', acc_mid='False', acc_v='False', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer_iwslt_de_en_v2', attention_dropout=0.1, batchnorm='True', best_checkpoint_metric='loss', bpe=None, bucket_cap_mb=25, clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/yefan0726/data/nlp/mt/data-bin/iwslt14.tokenized.de-en.joined', dataset_impl=None, ddp_backend='no_c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_norm_ff='layer', decoder_norm_self='layer', decoder_normalize_before=True, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, dropout_gama=0.5, dropout_type='none', early_stop=10000, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_norm_ff='layer', encoder_norm_self='layer', encoder_normalize_before=True, encoder_spec_norm=False, esd_interval=500, fast_stat_sync=False, filter_zeros='False', find_unused_parameters=False, fix_batches_to_gpus=False, fix_fingers='xmin_mid', fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=5, label_smoothing=0.1, layer_wise_attention=False, lazy_load=False, left_pad_source='True', left_pad_target='False', load_alignments=False, log_format=None, log_interval=50, lr=[0.0015], lr_min_ratio=0.6, lr_scale=2.0, lr_scheduler='inverse_sqrt', lr_slope=0.8, max_epoch=55, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, metric='alpha', min_loss_scale=0.0001, min_lr=1e-09, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_token_positional_embeddings=False, num_workers=1, optimizer='adam', optimizer_overrides='{}', raw_text=False, relu_dropout=0.1, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='/data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0/checkpoint_best.pt', save_dir='/data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0', save_interval=1, save_interval_updates=0, seed=43, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, single_gpu=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tb='False', tbmf_wrapper=False, tbr_after_warm='True', temp_balance_lr='tb_linear_map', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', update_freq=[1], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_init_lr=1e-07, warmup_updates=8000, weight_decay=0.0001, xmin_pos=2.0)
| [de] dictionary: 10152 types
| [en] dictionary: 10152 types
| loaded 7283 examples from: /data/yefan0726/data/nlp/mt/data-bin/iwslt14.tokenized.de-en.joined/valid.de-en.de
| loaded 7283 examples from: /data/yefan0726/data/nlp/mt/data-bin/iwslt14.tokenized.de-en.joined/valid.de-en.en
| /data/yefan0726/data/nlp/mt/data-bin/iwslt14.tokenized.de-en.joined valid de-en 7283 examples
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
| model transformer_iwslt_de_en_v2, criterion LabelSmoothedCrossEntropyCriterion
| num. model params: 36743168 (num. trained: 36743168)
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
| epoch 014:    900 / 1101 loss=6.198, nll_loss=5.030, ppl=32.68, wps=17470, ups=5, wpb=3589.751, bsz=144.790, num_updates=15214, lr=0.00999999, gnorm=0.736, clip=0.000, oom=0.000, wall=189, train_wall=2992
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
| epoch 014:    950 / 1101 loss=6.194, nll_loss=5.025, ppl=32.56, wps=17409, ups=5, wpb=3586.920, bsz=144.108, num_updates=15264, lr=0.00999999, gnorm=0.735, clip=0.000, oom=0.000, wall=200, train_wall=3003
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
| epoch 014:   1000 / 1101 loss=6.175, nll_loss=5.003, ppl=32.07, wps=17366, ups=5, wpb=3587.905, bsz=145.414, num_updates=15314, lr=0.00999999, gnorm=0.738, clip=0.000, oom=0.000, wall=211, train_wall=3014
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
| epoch 014:   1050 / 1101 loss=6.162, nll_loss=4.989, ppl=31.76, wps=17324, ups=5, wpb=3588.667, bsz=145.552, num_updates=15364, lr=0.00999999, gnorm=0.741, clip=0.000, oom=0.000, wall=222, train_wall=3024
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
| epoch 014:   1100 / 1101 loss=6.155, nll_loss=4.982, ppl=31.59, wps=17362, ups=5, wpb=3586.843, bsz=145.540, num_updates=15414, lr=0.00999999, gnorm=0.745, clip=0.000, oom=0.000, wall=232, train_wall=3034
| epoch 014 | loss 6.155 | nll_loss 4.982 | ppl 31.59 | wps 17355 | ups 5 | wpb 3586.843 | bsz 145.540 | num_updates 15414 | lr 0.00999999 | gnorm 0.745 | clip 0.000 | oom 0.000 | wall 232 | train_wall 3034
| epoch 014 | valid on 'valid' subset | loss 5.636 | nll_loss 4.328 | ppl 20.08 | num_updates 15414 | best_loss 4.06115
| Not the best ckpt... not best: 1
Scheduling with block acceleration
----------> number of V layers: 6
| saved checkpoint /data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0/checkpoint14.pt (epoch 14 @ 15414 updates) (writing took 3.376744270324707 seconds)
Remove old epoch checkpoints
Remove /data/yefan0726/checkpoints/zihang/checkpoints/nlp/mt/iwslt14_de_en/adam_acc_block/transformer_iwslt_de_en_v2_iwslt14_de_en_seed43/lr_scale_2.0/checkpoint9.pt
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
| epoch 015:     50 / 1101 loss=5.961, nll_loss=4.755, ppl=26.99, wps=18222, ups=5, wpb=3466.059, bsz=135.686, num_updates=15465, lr=0.00999999, gnorm=0.899, clip=0.000, oom=0.000, wall=248, train_wall=3043
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
--------------------> Update the metrics values
=================================
fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
--------------------> Compute cumulative weight changes
=================================
anslyzing the cumulated weight matrices: fix_fingers: xmin_mid, xmin_pos: 2.0, conv_norm: 0.5, filter_zeros: False
=================================
Scheduling with block acceleration
----------> number of V layers: 6
--------------------> Save the layer stats
# of layers in model: 96, # of layers in optimizer: 0, # of layers in esd analysis: 96
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
| epoch 015:    100 / 1101 loss=5.921, nll_loss=4.710, ppl=26.17, wps=13832, ups=4, wpb=3475.218, bsz=141.069, num_updates=15515, lr=0.00999999, gnorm=0.856, clip=0.000, oom=0.000, wall=264, train_wall=3058
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
Scheduling with block acceleration
----------> number of V layers: 6
